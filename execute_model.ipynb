{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from User_based_CF import *\n",
    "from Item_based_CF import *\n",
    "from Matrix_Factorization import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix(data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    data: (user_column_name, item_column_name, result_name, timestamp)\n",
    "    \"\"\"\n",
    "    user_list = rating_data.iloc[:, 0].values\n",
    "    item_list = rating_data[item_column_name].iloc[:, 0].values\n",
    "    rating_list = rating_data[result_name].values\n",
    "    user_item_matrix_data = pd.crosstab(index=user_list, columns=item_list, values=rating_list, aggfunc=np.mean,\\\n",
    "        rownames=[list(data.index)], colnames=[list(data.columns)])\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify whether the value exists or not.\n",
    "def identify_value_exist(user_item_matrix_data):\n",
    "    \"\"\"\n",
    "    user_item_matrix_data: DataFrame\n",
    "    \"\"\"\n",
    "    return (user_item_matrix_data.isna() == False).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix_for_matrix_factorization(data, unique_user_id, unique_item_id):\n",
    "    \"\"\"\n",
    "    data: (user, item, rating, timestamp)\n",
    "    \"\"\"\n",
    "    user_item_matrix_data = pd.DataFrame(np.array([np.nan] * (len(unique_user_id) * len(unique_item_id))).reshape(len(unique_user_id), len(unique_item_id)),\\\n",
    "        index=unique_user_id, columns=unique_item_id)\n",
    "    \n",
    "    for one_index in data.index:\n",
    "        user_item_matrix_data.loc[data.loc[one_index, \"User_id\"], data.loc[one_index, \"Item_id\"]] = \\\n",
    "            data.loc[one_index, \"Rating\"]\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要建構四種資料，分別為User的特徵、Item的特徵、User-Item matrix與User對應Item的紀錄\n",
    "def split_four_data(user_data, item_data, user_item_interaction_data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    user_data：使用者相關資料（user_id一定要放第一個column）\n",
    "    item_data：物品相關資料（item_id一定要放第二個column）\n",
    "    \"\"\"\n",
    "    all_data = list()\n",
    "    if user_data != None:\n",
    "        user_feature_data = user_data.iloc[:, 1:]\n",
    "        all_data.append(user_feature_data)\n",
    "    \n",
    "    if item_data != None:\n",
    "        item_feature_data = item_data.iloc[:, 1:]\n",
    "        all_data.append(item_feature_data)\n",
    "    \n",
    "    if user_item_interaction_data != None:\n",
    "        # transform train data into user-item matrix\n",
    "        user_item_matrix_data = create_user_item_matrix(user_item_interaction_data, user_column_name, item_column_name, result_name)\n",
    "        all_data.append(user_item_interaction_data)\n",
    "        all_data.append(user_item_matrix_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將每種不同資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\Movielens\\movie_genre.dat\", \"r\") as f:\n",
    "    movie_genre = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_genre = pd.DataFrame(np.array(movie_genre), columns=[\"movie_id\", \"genre\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\movie_movie(knn).dat\", \"r\") as f:\n",
    "    movie_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_movie = pd.DataFrame(np.array(movie_movie), columns=[\"movie1\", \"movie2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_age.dat\", \"r\") as f:\n",
    "    user_age = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_age = pd.DataFrame(np.array(user_age), columns=[\"user_id\", \"age\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_occupation.dat\", \"r\") as f:\n",
    "    user_occupation = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_occupation = pd.DataFrame(np.array(user_occupation), columns=[\"user_id\", \"occupation\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_user(knn).dat\", \"r\") as f:\n",
    "    user_user = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_user = pd.DataFrame(np.array(user_user), columns=[\"user1\", \"user2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_movie.dat\", \"r\") as f:\n",
    "    user_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_movie = pd.DataFrame(np.array(user_movie), columns=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.merge(user_movie, user_age, how=\"left\", on=\"user_id\")\n",
    "merge_data = pd.merge(merge_data, user_occupation, how=\"left\", on=\"user_id\")\n",
    "merge_data = pd.merge(merge_data, movie_genre, how=\"left\", on=\"movie_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_data = merge_data[[\"age\", \"occupation\"]]\n",
    "movie_feature_data = merge_data[[\"genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生四種資料\n",
    "user_feature_data, movie_feature_data, user_item_interaction_data, user_item_matrix_data =\\\n",
    "     split_four_data(user_feature_data, movie_feature_data, user_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Douban_Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Item_id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Item_id  Rating           timestamp\n",
       "0      196      242       3 1997-12-04 15:55:49\n",
       "1      186      302       3 1998-04-04 19:22:22\n",
       "2       22      377       1 1997-11-07 07:18:36\n",
       "3      244       51       2 1997-11-27 05:02:03\n",
       "4      166      346       1 1998-02-02 05:33:16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data to dataframe\n",
    "with open(\"ratings.data\", \"r\") as f:\n",
    "    rating_data = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "rating_data = pd.DataFrame(np.array(rating_data), columns=[\"User_id\", \"Item_id\", \"Rating\", \"timestamp\"]).astype(\"int\")\n",
    "\n",
    "# transform timestamp into datetime\n",
    "rating_data[\"timestamp\"] = [datetime.utcfromtimestamp(i) for i in rating_data[\"timestamp\"]]\n",
    "rating_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "traindata, testdata = train_test_split(rating_data, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生四種資料\n",
    "user_feature_data, movie_feature_data, user_item_interaction_data, user_item_matrix_data = split_four_data(user_data, item_data, user_item_interaction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 889249/889249 [04:04<00:00, 3631.08it/s]\n",
      "100%|██████████| 25000/25000 [13:46<00:00, 30.25it/s]\n",
      "100%|██████████| 25000/25000 [13:57<00:00, 29.85it/s]\n",
      "100%|██████████| 25000/25000 [14:30<00:00, 28.72it/s]\n",
      "100%|██████████| 25000/25000 [15:21<00:00, 27.12it/s]\n",
      "100%|██████████| 25000/25000 [15:51<00:00, 26.28it/s]\n",
      "100%|██████████| 25000/25000 [16:22<00:00, 25.45it/s]\n",
      "100%|██████████| 25000/25000 [16:56<00:00, 24.59it/s]\n",
      "100%|██████████| 25000/25000 [17:57<00:00, 23.21it/s]\n",
      "100%|██████████| 25000/25000 [22:41<00:00, 18.37it/s]\n",
      "100%|██████████| 25000/25000 [27:04<00:00, 15.39it/s]\n",
      "100%|██████████| 25000/25000 [32:40<00:00, 12.75it/s]\n",
      "100%|██████████| 25000/25000 [37:48<00:00, 11.02it/s]\n",
      "100%|██████████| 25000/25000 [44:33<00:00,  9.35it/s]\n",
      "100%|██████████| 25000/25000 [49:31<00:00,  8.41it/s]\n",
      "100%|██████████| 25000/25000 [53:32<00:00,  7.78it/s]\n",
      "100%|██████████| 25000/25000 [1:00:04<00:00,  6.94it/s]\n",
      "100%|██████████| 25000/25000 [1:07:35<00:00,  6.16it/s]\n",
      "100%|██████████| 2829124/2829124 [04:07<00:00, 11434.30it/s]\n",
      "100%|██████████| 25000/25000 [13:43<00:00, 30.37it/s]\n",
      "100%|██████████| 25000/25000 [14:35<00:00, 28.55it/s]\n",
      "100%|██████████| 25000/25000 [15:09<00:00, 27.49it/s]\n",
      "100%|██████████| 25000/25000 [16:20<00:00, 25.50it/s]\n",
      "100%|██████████| 25000/25000 [16:35<00:00, 25.10it/s]\n",
      "100%|██████████| 25000/25000 [16:29<00:00, 25.28it/s]\n",
      "100%|██████████| 25000/25000 [16:45<00:00, 24.87it/s]\n",
      "100%|██████████| 25000/25000 [16:56<00:00, 24.59it/s]\n",
      "100%|██████████| 25000/25000 [21:48<00:00, 19.10it/s]\n",
      "100%|██████████| 25000/25000 [26:34<00:00, 15.68it/s]\n",
      "100%|██████████| 25000/25000 [31:35<00:00, 13.19it/s]\n",
      "100%|██████████| 25000/25000 [36:14<00:00, 11.50it/s]\n",
      "100%|██████████| 25000/25000 [43:46<00:00,  9.52it/s]\n",
      "  1%|          | 232/25000 [00:26<48:01,  8.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fa635400262e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muser_cf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_without_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_user_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mresult_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"item-based_{one_similarity_method}_{K}\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Rating\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_user_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fa635400262e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muser_cf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_without_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_user_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mresult_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"item-based_{one_similarity_method}_{K}\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Rating\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_user_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github_RecSys\\Recommandation_System\\User_based_CF.py\u001b[0m in \u001b[0;36mpredict_without_time\u001b[1;34m(self, user_id, item_id, num_user)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 2. 找到某個相似的人中針對某個item的rating與時間\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mpredict_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Item_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 3. 計算分母與分子\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github_RecSys\\Recommandation_System\\User_based_CF.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 2. 找到某個相似的人中針對某個item的rating與時間\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mpredict_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Item_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 3. 計算分母與分子\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarity_method = [\"pearson\", \"cosine\"]\n",
    "K_list = [3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "CF_result = dict()\n",
    "\n",
    "for one_similarity_method in similarity_method:\n",
    "    # User-based Collaborative Filtering\n",
    "    user_cf = User_based_CF(traindata, user_item_matrix_data)\n",
    "    user_user_correlation_data = user_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "\n",
    "    for K in K_list:\n",
    "        # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "        pred_user_data = list(map(lambda x: user_cf.predict_without_time(testdata.iloc[x, 0], testdata.iloc[x, 1], num_user=K), tqdm([i for i in range(testdata.shape[0])])))\n",
    "        pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "        CF_result[f\"user-based_{one_similarity_method}_{K}\"] = math.sqrt(mean_squared_error(y_true=testdata[\"Rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "\n",
    "    # Item-based Collaborative Filtering\n",
    "    item_cf = Item_based_CF(traindata, user_item_matrix_data)\n",
    "    item_item_correlation_data = item_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "\n",
    "    for K in K_list:\n",
    "        # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "        pred_user_data = list(map(lambda x: user_cf.predict_without_time(testdata.iloc[x, 0], testdata.iloc[x, 1], num_user=K), tqdm([i for i in range(testdata.shape[0])])))\n",
    "        pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "        CF_result[f\"item-based_{one_similarity_method}_{K}\"] = math.sqrt(mean_squared_error(y_true=testdata[\"Rating\"].values, y_pred=np.array(pred_user_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, true_user_item_matrix, num_features):\n",
    "        \"\"\"\n",
    "        true_user_item_matrix：user與item的matrix，沒有補過值\n",
    "        \"\"\"\n",
    "        self.user_id_list = list(true_user_item_matrix.index)\n",
    "        self.item_id_list = list(true_user_item_matrix.columns)\n",
    "\n",
    "        # 辨識該值是否真的有值\n",
    "        self.identify_value_exist = torch.from_numpy( user_item_matrix_data.isna().astype(\"float\").values )\n",
    "        self.true_user_item_matrix = self.preprocessing_user_item_matrix(true_user_item_matrix)\n",
    "\n",
    "        self.p_matrix = torch.randn(size=(len(self.user_id_list), num_features), requires_grad=True)\n",
    "        self.q_matrix = torch.randn(size=(num_features, len(self.item_id_list)), requires_grad=True)\n",
    "\n",
    "        # 計算global mean\n",
    "        self.global_mean = torch.mean( self.true_user_item_matrix.flatten()[self.true_user_item_matrix.flatten().nonzero()] )\n",
    "\n",
    "        # 計算bias of user and bias of item\n",
    "        self.bu = torch.Tensor(list(map(lambda x: torch.mean(x[x.nonzero()]), self.true_user_item_matrix ))).reshape(shape=(-1, 1)) - self.global_mean\n",
    "        self.bi = torch.Tensor(list(map(lambda x: torch.mean(x[x.nonzero()]), torch.transpose(self.true_user_item_matrix, 0, 1) ))).reshape(shape=(1, -1)) - self.global_mean\n",
    "        return\n",
    "\n",
    "    def preprocessing_user_item_matrix(self, true_user_item_matrix):\n",
    "        # 把NaN全部補零\n",
    "        fill_user_item_matrix_data = true_user_item_matrix.fillna(0)\n",
    "        return torch.from_numpy(fill_user_item_matrix_data.values)\n",
    "    \n",
    "    def fit(self, epochs, learning_rate, regularization_rate, bias_or_not):\n",
    "        # 建立空的儲存以存取Loss\n",
    "        self.train_loss = list()\n",
    "\n",
    "        # 定義loss function\n",
    "        loss_func = nn.MSELoss()\n",
    "\n",
    "        # 定義optimizer\n",
    "        optimizer = torch.optim.SGD([self.p_matrix, self.q_matrix], lr=learning_rate, weight_decay=regularization_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if bias_or_not == False:\n",
    "                yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))\n",
    "            else:\n",
    "                yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))+self.bu+self.bi+self.global_mean\n",
    "                \n",
    "            yhat = yhat * identify_value_exist\n",
    "\n",
    "            loss = loss_func(yhat, self.true_user_item_matrix)\n",
    "            self.train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"=== Train Loss: {loss.item()}\")\n",
    "        return\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))\n",
    "        yhat_dataframe = pd.DataFrame(yhat.detach().numpy(), index=self.user_id_list, columns=self.item_id_list)\n",
    "        return yhat_dataframe.loc[user_id, item_id]\n",
    "\n",
    "    def evaluate(self, testdata):\n",
    "        \"\"\"\n",
    "        testdata：data.frame，<user_id, item_id, rating, (timestamp)>\n",
    "        \"\"\"\n",
    "        testdata[\"yhat\"] = list(map(lambda user, item: self.predict(user, item), testdata.iloc[:, 0], testdata.iloc[:, 1]))\n",
    "        print(f\"MSE: {mean_squared_error(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\\nr2_score: {r2_score(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\")\n",
    "        return\n",
    "\n",
    "    def save_model(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "num_user_id = user_item_matrix_data.shape[0]\n",
    "num_item_id = user_item_matrix_data.shape[1]\n",
    "num_features = 10\n",
    "\n",
    "model = matrix_factorization(true_user_item_matrix=user_item_matrix_data, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 Train Loss: 20.347803903557356\n",
      "=== Epoch: 1 Train Loss: 20.343315536402333\n",
      "=== Epoch: 2 Train Loss: 20.33882939400865\n",
      "=== Epoch: 3 Train Loss: 20.33434546393661\n",
      "=== Epoch: 4 Train Loss: 20.329863742936592\n",
      "=== Epoch: 5 Train Loss: 20.325384238194857\n",
      "=== Epoch: 6 Train Loss: 20.32090698039349\n",
      "=== Epoch: 7 Train Loss: 20.31643194274621\n",
      "=== Epoch: 8 Train Loss: 20.311959118716747\n",
      "=== Epoch: 9 Train Loss: 20.307488476059742\n",
      "=== Epoch: 10 Train Loss: 20.30302005638681\n",
      "=== Epoch: 11 Train Loss: 20.29855385718001\n",
      "=== Epoch: 12 Train Loss: 20.29408990454399\n",
      "=== Epoch: 13 Train Loss: 20.289628141663844\n",
      "=== Epoch: 14 Train Loss: 20.285168583568062\n",
      "=== Epoch: 15 Train Loss: 20.280711219793393\n",
      "=== Epoch: 16 Train Loss: 20.276256088627882\n",
      "=== Epoch: 17 Train Loss: 20.271803161762897\n",
      "=== Epoch: 18 Train Loss: 20.267352438306993\n",
      "=== Epoch: 19 Train Loss: 20.262903901035056\n",
      "=== Epoch: 20 Train Loss: 20.258457561614453\n",
      "=== Epoch: 21 Train Loss: 20.254013430075798\n",
      "=== Epoch: 22 Train Loss: 20.249571528636828\n",
      "=== Epoch: 23 Train Loss: 20.24513182419318\n",
      "=== Epoch: 24 Train Loss: 20.240694277507263\n",
      "=== Epoch: 25 Train Loss: 20.236258958777167\n",
      "=== Epoch: 26 Train Loss: 20.231825835831746\n",
      "=== Epoch: 27 Train Loss: 20.227394907999575\n",
      "=== Epoch: 28 Train Loss: 20.222966142960882\n",
      "=== Epoch: 29 Train Loss: 20.21853959170931\n",
      "=== Epoch: 30 Train Loss: 20.214115217791516\n",
      "=== Epoch: 31 Train Loss: 20.20969304288701\n",
      "=== Epoch: 32 Train Loss: 20.205273072825086\n",
      "=== Epoch: 33 Train Loss: 20.200855292927\n",
      "=== Epoch: 34 Train Loss: 20.19643966970183\n",
      "=== Epoch: 35 Train Loss: 20.192026238689543\n",
      "=== Epoch: 36 Train Loss: 20.187614985396557\n",
      "=== Epoch: 37 Train Loss: 20.183205940185427\n",
      "=== Epoch: 38 Train Loss: 20.178799099032798\n",
      "=== Epoch: 39 Train Loss: 20.174394411561448\n",
      "=== Epoch: 40 Train Loss: 20.16999190860599\n",
      "=== Epoch: 41 Train Loss: 20.16559154338408\n",
      "=== Epoch: 42 Train Loss: 20.161193391304735\n",
      "=== Epoch: 43 Train Loss: 20.15679741363838\n",
      "=== Epoch: 44 Train Loss: 20.15240359377509\n",
      "=== Epoch: 45 Train Loss: 20.148011972587142\n",
      "=== Epoch: 46 Train Loss: 20.14362251745521\n",
      "=== Epoch: 47 Train Loss: 20.139235225820702\n",
      "=== Epoch: 48 Train Loss: 20.134850089992685\n",
      "=== Epoch: 49 Train Loss: 20.130467126690505\n",
      "=== Epoch: 50 Train Loss: 20.126086336642544\n",
      "=== Epoch: 51 Train Loss: 20.12170772386598\n",
      "=== Epoch: 52 Train Loss: 20.117331297763666\n",
      "=== Epoch: 53 Train Loss: 20.11295701838151\n",
      "=== Epoch: 54 Train Loss: 20.108584885771602\n",
      "=== Epoch: 55 Train Loss: 20.104214898529474\n",
      "=== Epoch: 56 Train Loss: 20.099847098841327\n",
      "=== Epoch: 57 Train Loss: 20.095481447812855\n",
      "=== Epoch: 58 Train Loss: 20.09111796881777\n",
      "=== Epoch: 59 Train Loss: 20.086756632708006\n",
      "=== Epoch: 60 Train Loss: 20.08239746824066\n",
      "=== Epoch: 61 Train Loss: 20.078040440146957\n",
      "=== Epoch: 62 Train Loss: 20.073685568940277\n",
      "=== Epoch: 63 Train Loss: 20.069332866182883\n",
      "=== Epoch: 64 Train Loss: 20.064982307108806\n",
      "=== Epoch: 65 Train Loss: 20.060633892697144\n",
      "=== Epoch: 66 Train Loss: 20.056287613632655\n",
      "=== Epoch: 67 Train Loss: 20.051943511396352\n",
      "=== Epoch: 68 Train Loss: 20.047601541424182\n",
      "=== Epoch: 69 Train Loss: 20.04326170852461\n",
      "=== Epoch: 70 Train Loss: 20.038924037994533\n",
      "=== Epoch: 71 Train Loss: 20.03458848751771\n",
      "=== Epoch: 72 Train Loss: 20.030255080815497\n",
      "=== Epoch: 73 Train Loss: 20.02592383814292\n",
      "=== Epoch: 74 Train Loss: 20.021594707957963\n",
      "=== Epoch: 75 Train Loss: 20.017267742710835\n",
      "=== Epoch: 76 Train Loss: 20.01294290982437\n",
      "=== Epoch: 77 Train Loss: 20.00862020481445\n",
      "=== Epoch: 78 Train Loss: 20.004299628355245\n",
      "=== Epoch: 79 Train Loss: 19.999981177930707\n",
      "=== Epoch: 80 Train Loss: 19.995664887548493\n",
      "=== Epoch: 81 Train Loss: 19.991350722541906\n",
      "=== Epoch: 82 Train Loss: 19.98703868669361\n",
      "=== Epoch: 83 Train Loss: 19.982728753116913\n",
      "=== Epoch: 84 Train Loss: 19.978420960108558\n",
      "=== Epoch: 85 Train Loss: 19.974115309635195\n",
      "=== Epoch: 86 Train Loss: 19.969811795255623\n",
      "=== Epoch: 87 Train Loss: 19.965510399627643\n",
      "=== Epoch: 88 Train Loss: 19.961211116232143\n",
      "=== Epoch: 89 Train Loss: 19.956913942845073\n",
      "=== Epoch: 90 Train Loss: 19.95261889363994\n",
      "=== Epoch: 91 Train Loss: 19.94832598555271\n",
      "=== Epoch: 92 Train Loss: 19.944035182899967\n",
      "=== Epoch: 93 Train Loss: 19.939746495545933\n",
      "=== Epoch: 94 Train Loss: 19.935459957360315\n",
      "=== Epoch: 95 Train Loss: 19.931175506030534\n",
      "=== Epoch: 96 Train Loss: 19.92689315971543\n",
      "=== Epoch: 97 Train Loss: 19.92261294110585\n",
      "=== Epoch: 98 Train Loss: 19.918334846896677\n",
      "=== Epoch: 99 Train Loss: 19.914058850957787\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs=epochs, learning_rate=learning_rate, regularization_rate=1e-2, bias_or_not=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:53<00:00, 463.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 24.362353662799624\n",
      "r2_score: -18.258546776614086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(testdata=testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入資料的結構：<user_id, item_id, result, timestamp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有東西都變成Label Encoding→Movielens\n",
    "def movielens_onehotencoding(user_feature_data, movie_feature_data):\n",
    "    user_age_onehotcoding = OneHotEncoder().fit(user_feature_data[\"user_age\"])\n",
    "    user_occupation_onehotencoding = OneHotEncoder().fit(user_feature_data[\"user_occupation\"])\n",
    "    movie_genre_onehotencoding = OneHotEncoder().fit(movie_feature_data[\"movie_genre\"])\n",
    "    return user_age_onehotcoding, user_occupation_onehotencoding, movie_genre_onehotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建構模型→Movielens\n",
    "class fm_model(nn.Module):\n",
    "    def __init__(self, num_user_age, num_user_occupation, num_movie_genre, num_features):\n",
    "        super(fm_model, self).__init__()\n",
    "        self.user_age = nn.Linear(num_user_age, num_features)\n",
    "        self.user_occupation = nn.Linear(num_user_occupation, num_features)\n",
    "        self.movie_genre = nn.Linear(num_movie_genre, num_features)\n",
    "        self.user_age_weight_linear = nn.Linear(num_user_age, 1)\n",
    "        self.user_occupation_weight_linear = nn.Linear(num_user_occupation, 1)\n",
    "        self.movie_genre_weight_linear = nn.Linear(num_movie_genre, 1)\n",
    "        self.decoder = nn.Linear(6, 1)\n",
    "        return\n",
    "\n",
    "    def forward(self, user_age_feature, user_occupation_feature, movie_genre_feature):\n",
    "        # Embedding Learning\n",
    "        self.user_age_embedding = self.user_age(user_age_feature) # shape=(batch_size, num_features)\n",
    "        self.user_occupation_embedding = self.user_occupation(user_occupation_feature) # shape=(batch_size, num_features)\n",
    "        self.movie_genre_embedding = self.movie_genre(movie_genre_feature) # shape=(batch_size, num_features)\n",
    "        self.user_age_weight = self.user_age_weight_linear(user_age_feature) # shape = (batch_size, 1)\n",
    "        self.user_occupation_weight = self.user_occupation_weight_linear(user_occupation_feature) # shape = (batch_size, 1)\n",
    "        self.movie_genre_weight = self.movie_genre_weight_linear(movie_genre_feature) # shape = (batch_size, 1)\n",
    "\n",
    "        # Inner product\n",
    "        self.user_age_user_occupation = self.user_age_embedding * self.user_occupation_embedding # shape=(batch_size, num_features)\n",
    "        self.user_age_movie_genre = self.user_age_embedding * self.movie_genre_embedding # shape=(batch_size, num_features)\n",
    "        self.user_occupation_movie_genre = self.user_occupation_embedding * self.movie_genre_embedding # shape=(batch_size, num_features)\n",
    "\n",
    "        # Concatenate\n",
    "        self.all = torch.cat((self.user_age_user_occupation, self.user_age_movie_genre, self.user_occupation_movie_genre,\\\n",
    "                              self.user_age_weight, self.user_occupation_weight, self.movie_genre_weight), dim=-1) \n",
    "\n",
    "        # Decoder\n",
    "        X = self.decoder(self.all)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDBT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "338435f3ace29f4c96579772b339886e16b2e32c5ce1705983594e7aef710dc8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
