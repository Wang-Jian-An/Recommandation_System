{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from User_based_CF import *\n",
    "from Item_based_CF import *\n",
    "from Matrix_Factorization import *\n",
    "from Factorization_Machine import *\n",
    "from IPNN_model import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix(data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    data: (user_column_name, item_column_name, result_name, timestamp)\n",
    "    \"\"\"\n",
    "    user_list = data.iloc[:, 0]\n",
    "    item_list = data.iloc[:, 1]\n",
    "    rating_list = data[result_name].values\n",
    "    user_item_matrix_data = pd.crosstab(index=user_list, columns=item_list, values=rating_list, aggfunc=np.mean)\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify whether the value exists or not.\n",
    "def identify_value_exist(user_item_matrix_data):\n",
    "    \"\"\"\n",
    "    user_item_matrix_data: DataFrame\n",
    "    \"\"\"\n",
    "    return (user_item_matrix_data.isna() == False).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix_for_matrix_factorization(data, unique_user_id, unique_item_id):\n",
    "    \"\"\"\n",
    "    data: (user, item, rating, timestamp)\n",
    "    \"\"\"\n",
    "    user_item_matrix_data = pd.DataFrame(np.array([np.nan] * (len(unique_user_id) * len(unique_item_id))).reshape(len(unique_user_id), len(unique_item_id)),\\\n",
    "        index=unique_user_id, columns=unique_item_id)\n",
    "    \n",
    "    for one_index in data.index:\n",
    "        user_item_matrix_data.loc[data.loc[one_index, \"User_id\"], data.loc[one_index, \"Item_id\"]] = \\\n",
    "            data.loc[one_index, \"Rating\"]\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要建構四種資料，分別為User的特徵、Item的特徵、User-Item matrix與User對應Item的紀錄\n",
    "def split_four_data(user_data, item_data, user_item_interaction_data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    user_data：使用者相關資料（user_id一定要放第一個column）\n",
    "    item_data：物品相關資料（item_id一定要放第二個column）\n",
    "    \"\"\"\n",
    "    all_data = list()\n",
    "    if isinstance(user_data, pd.DataFrame):\n",
    "        # user_feature_data = user_data.iloc[:, 1:]\n",
    "        all_data.append(user_data)\n",
    "    \n",
    "    if isinstance(item_data, pd.DataFrame):\n",
    "        # item_feature_data = item_data.iloc[:, 1:]\n",
    "        all_data.append(item_data)\n",
    "    \n",
    "    if isinstance(user_item_interaction_data, pd.DataFrame):\n",
    "        # transform train data into user-item matrix\n",
    "        user_item_matrix_data = create_user_item_matrix(user_item_interaction_data, user_column_name, item_column_name, result_name)\n",
    "        all_data.append(user_item_interaction_data)\n",
    "        all_data.append(user_item_matrix_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將每種不同資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義OneHotEncoding的內容→Movielens\n",
    "def movielens_onehotencoding(user_feature_data, movie_feature_data):\n",
    "    user_age_onehotcoding = OneHotEncoder(sparse=False).fit(user_feature_data[\"age\"].values.reshape((-1, 1)))\n",
    "    user_occupation_onehotencoding = OneHotEncoder(sparse=False).fit(user_feature_data[\"occupation\"].values.reshape((-1, 1)))\n",
    "    return user_age_onehotcoding, user_occupation_onehotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\Movielens\\movie_genre.dat\", \"r\") as f:\n",
    "    movie_genre = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_genre = pd.DataFrame(np.array(movie_genre), columns=[\"movie_id\", \"genre\"])\n",
    "movie_genre[\"genre\"] = movie_genre[\"genre\"].astype(\"str\")\n",
    "\n",
    "with open(r\"data\\Movielens\\movie_movie(knn).dat\", \"r\") as f:\n",
    "    movie_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_movie = pd.DataFrame(np.array(movie_movie), columns=[\"movie1\", \"movie2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_age.dat\", \"r\") as f:\n",
    "    user_age = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_age = pd.DataFrame(np.array(user_age), columns=[\"user_id\", \"age\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_occupation.dat\", \"r\") as f:\n",
    "    user_occupation = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_occupation = pd.DataFrame(np.array(user_occupation), columns=[\"user_id\", \"occupation\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_user(knn).dat\", \"r\") as f:\n",
    "    user_user = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_user = pd.DataFrame(np.array(user_user), columns=[\"user1\", \"user2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_movie.dat\", \"r\") as f:\n",
    "    user_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_movie = pd.DataFrame(np.array(user_movie), columns=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"])\n",
    "user_movie[\"rating\"] = user_movie[\"rating\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 針對電影種類前處理：由於一部電影可能有多種種類，因此將每個種類用OneHotEncoding表示\n",
    "movie_genre[\"index\"] = 1\n",
    "movie_genre = movie_genre.pivot_table(index=\"movie_id\", columns=\"genre\", values=\"index\", fill_value=0)\n",
    "movie_genre = movie_genre.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n",
      "(100000, 6)\n",
      "(100000, 24)\n"
     ]
    }
   ],
   "source": [
    "merge_data = pd.merge(user_movie, user_age, how=\"inner\", on=\"user_id\")\n",
    "print(merge_data.shape)\n",
    "merge_data = pd.merge(merge_data, user_occupation, how=\"left\", on=\"user_id\")\n",
    "print(merge_data.shape)\n",
    "merge_data = pd.merge(merge_data, movie_genre, how=\"left\", on=\"movie_id\").fillna(0)\n",
    "print(merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_data = merge_data[[\"age\", \"occupation\"]]\n",
    "movie_feature_data = merge_data[movie_genre.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生四種資料\n",
    "user_feature_data, movie_feature_data, user_item_interaction_data, user_item_matrix_data =\\\n",
    "     split_four_data(user_feature_data, movie_feature_data, user_movie, user_column_name=\"user_id\", item_column_name=\"movie_id\", result_name=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把四種資料作訓練與測試資料的切割\n",
    "train_user_feature_data, test_user_feature_data, train_movie_feature_data, test_movie_feature_data = \\\n",
    "    train_test_split(user_feature_data, movie_feature_data, random_state=12345, test_size=0.25)\n",
    "\n",
    "train_result_data, test_result_data =\\\n",
    "    train_test_split(user_item_interaction_data[\"rating\"].values, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定義OneHotEncoding的內容→Movielens\n",
    "user_age_onehotencoding, user_occupation_onehotencoding =\\\n",
    "    movielens_onehotencoding(user_feature_data, movie_feature_data)\n",
    "\n",
    "# 2. 把所有訓練資料以及測試資料都轉成OneHotEncoding\n",
    "train_user_age_onehotencoding, test_user_age_onehotencoding =\\\n",
    "    list(map(lambda x: user_age_onehotencoding.transform(x), [train_user_feature_data[\"age\"].values.reshape((-1, 1)), test_user_feature_data[\"age\"].values.reshape((-1, 1))]))\n",
    "train_user_occupation_onehotencoding, test_user_occupation_onehotencoding =\\\n",
    "    list(map(lambda x: user_occupation_onehotencoding.transform(x), [train_user_feature_data[\"occupation\"].values.reshape((-1, 1)), test_user_feature_data[\"occupation\"].values.reshape((-1, 1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Douban_Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 889249/889249 [04:04<00:00, 3631.08it/s]\n",
      "100%|██████████| 25000/25000 [13:46<00:00, 30.25it/s]\n",
      "100%|██████████| 25000/25000 [13:57<00:00, 29.85it/s]\n",
      "100%|██████████| 25000/25000 [14:30<00:00, 28.72it/s]\n",
      "100%|██████████| 25000/25000 [15:21<00:00, 27.12it/s]\n",
      "100%|██████████| 25000/25000 [15:51<00:00, 26.28it/s]\n",
      "100%|██████████| 25000/25000 [16:22<00:00, 25.45it/s]\n",
      "100%|██████████| 25000/25000 [16:56<00:00, 24.59it/s]\n",
      "100%|██████████| 25000/25000 [17:57<00:00, 23.21it/s]\n",
      "100%|██████████| 25000/25000 [22:41<00:00, 18.37it/s]\n",
      "100%|██████████| 25000/25000 [27:04<00:00, 15.39it/s]\n",
      "100%|██████████| 25000/25000 [32:40<00:00, 12.75it/s]\n",
      "100%|██████████| 25000/25000 [37:48<00:00, 11.02it/s]\n",
      "100%|██████████| 25000/25000 [44:33<00:00,  9.35it/s]\n",
      "100%|██████████| 25000/25000 [49:31<00:00,  8.41it/s]\n",
      "100%|██████████| 25000/25000 [53:32<00:00,  7.78it/s]\n",
      "100%|██████████| 25000/25000 [1:00:04<00:00,  6.94it/s]\n",
      "100%|██████████| 25000/25000 [1:07:35<00:00,  6.16it/s]\n",
      "100%|██████████| 2829124/2829124 [04:07<00:00, 11434.30it/s]\n",
      "100%|██████████| 25000/25000 [13:43<00:00, 30.37it/s]\n",
      "100%|██████████| 25000/25000 [14:35<00:00, 28.55it/s]\n",
      "100%|██████████| 25000/25000 [15:09<00:00, 27.49it/s]\n",
      "100%|██████████| 25000/25000 [16:20<00:00, 25.50it/s]\n",
      "100%|██████████| 25000/25000 [16:35<00:00, 25.10it/s]\n",
      "100%|██████████| 25000/25000 [16:29<00:00, 25.28it/s]\n",
      "100%|██████████| 25000/25000 [16:45<00:00, 24.87it/s]\n",
      "100%|██████████| 25000/25000 [16:56<00:00, 24.59it/s]\n",
      "100%|██████████| 25000/25000 [21:48<00:00, 19.10it/s]\n",
      "100%|██████████| 25000/25000 [26:34<00:00, 15.68it/s]\n",
      "100%|██████████| 25000/25000 [31:35<00:00, 13.19it/s]\n",
      "100%|██████████| 25000/25000 [36:14<00:00, 11.50it/s]\n",
      "100%|██████████| 25000/25000 [43:46<00:00,  9.52it/s]\n",
      "  1%|          | 232/25000 [00:26<48:01,  8.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fa635400262e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muser_cf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_without_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_user_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mresult_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"item-based_{one_similarity_method}_{K}\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Rating\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_user_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fa635400262e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0muser_cf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_without_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mpred_user_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_user_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mresult_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"item-based_{one_similarity_method}_{K}\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Rating\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_user_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github_RecSys\\Recommandation_System\\User_based_CF.py\u001b[0m in \u001b[0;36mpredict_without_time\u001b[1;34m(self, user_id, item_id, num_user)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 2. 找到某個相似的人中針對某個item的rating與時間\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mpredict_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Item_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 3. 計算分母與分子\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Github_RecSys\\Recommandation_System\\User_based_CF.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# 2. 找到某個相似的人中針對某個item的rating與時間\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mpredict_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Item_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 3. 計算分母與分子\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarity_method = [\"pearson\", \"cosine\"]\n",
    "K_list = [3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "CF_result = dict()\n",
    "\n",
    "for one_similarity_method in similarity_method:\n",
    "    # User-based Collaborative Filtering\n",
    "    user_cf = User_based_CF(traindata, user_item_matrix_data)\n",
    "    user_user_correlation_data = user_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "\n",
    "    for K in K_list:\n",
    "        # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "        pred_user_data = list(map(lambda x: user_cf.predict_without_time(testdata.iloc[x, 0], testdata.iloc[x, 1], num_user=K), tqdm([i for i in range(testdata.shape[0])])))\n",
    "        pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "        CF_result[f\"user-based_{one_similarity_method}_{K}\"] = math.sqrt(mean_squared_error(y_true=testdata[\"Rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "\n",
    "    # Item-based Collaborative Filtering\n",
    "    item_cf = Item_based_CF(traindata, user_item_matrix_data)\n",
    "    item_item_correlation_data = item_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "\n",
    "    for K in K_list:\n",
    "        # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "        pred_user_data = list(map(lambda x: user_cf.predict_without_time(testdata.iloc[x, 0], testdata.iloc[x, 1], num_user=K), tqdm([i for i in range(testdata.shape[0])])))\n",
    "        pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "        CF_result[f\"item-based_{one_similarity_method}_{K}\"] = math.sqrt(mean_squared_error(y_true=testdata[\"Rating\"].values, y_pred=np.array(pred_user_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, true_user_item_matrix, num_features):\n",
    "        \"\"\"\n",
    "        true_user_item_matrix：user與item的matrix，沒有補過值\n",
    "        \"\"\"\n",
    "        self.user_id_list = list(true_user_item_matrix.index)\n",
    "        self.item_id_list = list(true_user_item_matrix.columns)\n",
    "\n",
    "        # 辨識該值是否真的有值\n",
    "        self.identify_value_exist = torch.from_numpy( user_item_matrix_data.isna().astype(\"float\").values )\n",
    "        self.true_user_item_matrix = self.preprocessing_user_item_matrix(true_user_item_matrix)\n",
    "\n",
    "        self.p_matrix = torch.randn(size=(len(self.user_id_list), num_features), requires_grad=True)\n",
    "        self.q_matrix = torch.randn(size=(num_features, len(self.item_id_list)), requires_grad=True)\n",
    "\n",
    "        # 計算global mean\n",
    "        self.global_mean = torch.mean( self.true_user_item_matrix.flatten()[self.true_user_item_matrix.flatten().nonzero()] )\n",
    "\n",
    "        # 計算bias of user and bias of item\n",
    "        self.bu = torch.Tensor(list(map(lambda x: torch.mean(x[x.nonzero()]), self.true_user_item_matrix ))).reshape(shape=(-1, 1)) - self.global_mean\n",
    "        self.bi = torch.Tensor(list(map(lambda x: torch.mean(x[x.nonzero()]), torch.transpose(self.true_user_item_matrix, 0, 1) ))).reshape(shape=(1, -1)) - self.global_mean\n",
    "        return\n",
    "\n",
    "    def preprocessing_user_item_matrix(self, true_user_item_matrix):\n",
    "        # 把NaN全部補零\n",
    "        fill_user_item_matrix_data = true_user_item_matrix.fillna(0)\n",
    "        return torch.from_numpy(fill_user_item_matrix_data.values)\n",
    "    \n",
    "    def fit(self, epochs, learning_rate, regularization_rate, bias_or_not):\n",
    "        # 建立空的儲存以存取Loss\n",
    "        self.train_loss = list()\n",
    "\n",
    "        # 定義loss function\n",
    "        loss_func = nn.MSELoss()\n",
    "\n",
    "        # 定義optimizer\n",
    "        optimizer = torch.optim.SGD([self.p_matrix, self.q_matrix], lr=learning_rate, weight_decay=regularization_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if bias_or_not == False:\n",
    "                yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))\n",
    "            else:\n",
    "                yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))+self.bu+self.bi+self.global_mean\n",
    "                \n",
    "            yhat = yhat * identify_value_exist\n",
    "\n",
    "            loss = loss_func(yhat, self.true_user_item_matrix)\n",
    "            self.train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"=== Train Loss: {loss.item()}\")\n",
    "        return\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        yhat = torch.tensordot(self.p_matrix, self.q_matrix, dims=([1], [0]))\n",
    "        yhat_dataframe = pd.DataFrame(yhat.detach().numpy(), index=self.user_id_list, columns=self.item_id_list)\n",
    "        return yhat_dataframe.loc[user_id, item_id]\n",
    "\n",
    "    def evaluate(self, testdata):\n",
    "        \"\"\"\n",
    "        testdata：data.frame，<user_id, item_id, rating, (timestamp)>\n",
    "        \"\"\"\n",
    "        testdata[\"yhat\"] = list(map(lambda user, item: self.predict(user, item), testdata.iloc[:, 0], testdata.iloc[:, 1]))\n",
    "        print(f\"MSE: {mean_squared_error(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\\nr2_score: {r2_score(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\")\n",
    "        return\n",
    "\n",
    "    def save_model(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "num_user_id = user_item_matrix_data.shape[0]\n",
    "num_item_id = user_item_matrix_data.shape[1]\n",
    "num_features = 10\n",
    "\n",
    "model = matrix_factorization(true_user_item_matrix=user_item_matrix_data, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 Train Loss: 20.347803903557356\n",
      "=== Epoch: 1 Train Loss: 20.343315536402333\n",
      "=== Epoch: 2 Train Loss: 20.33882939400865\n",
      "=== Epoch: 3 Train Loss: 20.33434546393661\n",
      "=== Epoch: 4 Train Loss: 20.329863742936592\n",
      "=== Epoch: 5 Train Loss: 20.325384238194857\n",
      "=== Epoch: 6 Train Loss: 20.32090698039349\n",
      "=== Epoch: 7 Train Loss: 20.31643194274621\n",
      "=== Epoch: 8 Train Loss: 20.311959118716747\n",
      "=== Epoch: 9 Train Loss: 20.307488476059742\n",
      "=== Epoch: 10 Train Loss: 20.30302005638681\n",
      "=== Epoch: 11 Train Loss: 20.29855385718001\n",
      "=== Epoch: 12 Train Loss: 20.29408990454399\n",
      "=== Epoch: 13 Train Loss: 20.289628141663844\n",
      "=== Epoch: 14 Train Loss: 20.285168583568062\n",
      "=== Epoch: 15 Train Loss: 20.280711219793393\n",
      "=== Epoch: 16 Train Loss: 20.276256088627882\n",
      "=== Epoch: 17 Train Loss: 20.271803161762897\n",
      "=== Epoch: 18 Train Loss: 20.267352438306993\n",
      "=== Epoch: 19 Train Loss: 20.262903901035056\n",
      "=== Epoch: 20 Train Loss: 20.258457561614453\n",
      "=== Epoch: 21 Train Loss: 20.254013430075798\n",
      "=== Epoch: 22 Train Loss: 20.249571528636828\n",
      "=== Epoch: 23 Train Loss: 20.24513182419318\n",
      "=== Epoch: 24 Train Loss: 20.240694277507263\n",
      "=== Epoch: 25 Train Loss: 20.236258958777167\n",
      "=== Epoch: 26 Train Loss: 20.231825835831746\n",
      "=== Epoch: 27 Train Loss: 20.227394907999575\n",
      "=== Epoch: 28 Train Loss: 20.222966142960882\n",
      "=== Epoch: 29 Train Loss: 20.21853959170931\n",
      "=== Epoch: 30 Train Loss: 20.214115217791516\n",
      "=== Epoch: 31 Train Loss: 20.20969304288701\n",
      "=== Epoch: 32 Train Loss: 20.205273072825086\n",
      "=== Epoch: 33 Train Loss: 20.200855292927\n",
      "=== Epoch: 34 Train Loss: 20.19643966970183\n",
      "=== Epoch: 35 Train Loss: 20.192026238689543\n",
      "=== Epoch: 36 Train Loss: 20.187614985396557\n",
      "=== Epoch: 37 Train Loss: 20.183205940185427\n",
      "=== Epoch: 38 Train Loss: 20.178799099032798\n",
      "=== Epoch: 39 Train Loss: 20.174394411561448\n",
      "=== Epoch: 40 Train Loss: 20.16999190860599\n",
      "=== Epoch: 41 Train Loss: 20.16559154338408\n",
      "=== Epoch: 42 Train Loss: 20.161193391304735\n",
      "=== Epoch: 43 Train Loss: 20.15679741363838\n",
      "=== Epoch: 44 Train Loss: 20.15240359377509\n",
      "=== Epoch: 45 Train Loss: 20.148011972587142\n",
      "=== Epoch: 46 Train Loss: 20.14362251745521\n",
      "=== Epoch: 47 Train Loss: 20.139235225820702\n",
      "=== Epoch: 48 Train Loss: 20.134850089992685\n",
      "=== Epoch: 49 Train Loss: 20.130467126690505\n",
      "=== Epoch: 50 Train Loss: 20.126086336642544\n",
      "=== Epoch: 51 Train Loss: 20.12170772386598\n",
      "=== Epoch: 52 Train Loss: 20.117331297763666\n",
      "=== Epoch: 53 Train Loss: 20.11295701838151\n",
      "=== Epoch: 54 Train Loss: 20.108584885771602\n",
      "=== Epoch: 55 Train Loss: 20.104214898529474\n",
      "=== Epoch: 56 Train Loss: 20.099847098841327\n",
      "=== Epoch: 57 Train Loss: 20.095481447812855\n",
      "=== Epoch: 58 Train Loss: 20.09111796881777\n",
      "=== Epoch: 59 Train Loss: 20.086756632708006\n",
      "=== Epoch: 60 Train Loss: 20.08239746824066\n",
      "=== Epoch: 61 Train Loss: 20.078040440146957\n",
      "=== Epoch: 62 Train Loss: 20.073685568940277\n",
      "=== Epoch: 63 Train Loss: 20.069332866182883\n",
      "=== Epoch: 64 Train Loss: 20.064982307108806\n",
      "=== Epoch: 65 Train Loss: 20.060633892697144\n",
      "=== Epoch: 66 Train Loss: 20.056287613632655\n",
      "=== Epoch: 67 Train Loss: 20.051943511396352\n",
      "=== Epoch: 68 Train Loss: 20.047601541424182\n",
      "=== Epoch: 69 Train Loss: 20.04326170852461\n",
      "=== Epoch: 70 Train Loss: 20.038924037994533\n",
      "=== Epoch: 71 Train Loss: 20.03458848751771\n",
      "=== Epoch: 72 Train Loss: 20.030255080815497\n",
      "=== Epoch: 73 Train Loss: 20.02592383814292\n",
      "=== Epoch: 74 Train Loss: 20.021594707957963\n",
      "=== Epoch: 75 Train Loss: 20.017267742710835\n",
      "=== Epoch: 76 Train Loss: 20.01294290982437\n",
      "=== Epoch: 77 Train Loss: 20.00862020481445\n",
      "=== Epoch: 78 Train Loss: 20.004299628355245\n",
      "=== Epoch: 79 Train Loss: 19.999981177930707\n",
      "=== Epoch: 80 Train Loss: 19.995664887548493\n",
      "=== Epoch: 81 Train Loss: 19.991350722541906\n",
      "=== Epoch: 82 Train Loss: 19.98703868669361\n",
      "=== Epoch: 83 Train Loss: 19.982728753116913\n",
      "=== Epoch: 84 Train Loss: 19.978420960108558\n",
      "=== Epoch: 85 Train Loss: 19.974115309635195\n",
      "=== Epoch: 86 Train Loss: 19.969811795255623\n",
      "=== Epoch: 87 Train Loss: 19.965510399627643\n",
      "=== Epoch: 88 Train Loss: 19.961211116232143\n",
      "=== Epoch: 89 Train Loss: 19.956913942845073\n",
      "=== Epoch: 90 Train Loss: 19.95261889363994\n",
      "=== Epoch: 91 Train Loss: 19.94832598555271\n",
      "=== Epoch: 92 Train Loss: 19.944035182899967\n",
      "=== Epoch: 93 Train Loss: 19.939746495545933\n",
      "=== Epoch: 94 Train Loss: 19.935459957360315\n",
      "=== Epoch: 95 Train Loss: 19.931175506030534\n",
      "=== Epoch: 96 Train Loss: 19.92689315971543\n",
      "=== Epoch: 97 Train Loss: 19.92261294110585\n",
      "=== Epoch: 98 Train Loss: 19.918334846896677\n",
      "=== Epoch: 99 Train Loss: 19.914058850957787\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs=epochs, learning_rate=learning_rate, regularization_rate=1e-2, bias_or_not=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:53<00:00, 463.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 24.362353662799624\n",
      "r2_score: -18.258546776614086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(testdata=testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_interaction的結構：<user_id, item_id, result, timestamp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 10\n",
    "\n",
    "# 1. 把所有東西包成dataloader\n",
    "train_dataset = TensorDataset( torch.FloatTensor(train_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(train_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(train_movie_feature_data.values),\n",
    "                               torch.FloatTensor(train_result_data) )\n",
    "test_dataset = TensorDataset(  torch.FloatTensor(test_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(test_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(test_movie_feature_data.values),\n",
    "                               torch.FloatTensor(test_result_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "# 2. 呼叫模型與設定Loss function\n",
    "model = fm_model(num_user_age=train_user_age_onehotencoding.shape[1], \n",
    "                 num_user_occupation=train_user_occupation_onehotencoding.shape[1], \n",
    "                 num_movie_genre=train_movie_feature_data.values.shape[1],\n",
    "                 num_decoder=num_features*3+3,\n",
    "                 num_features=num_features)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0, Train Loss: 2425.652796149254\n",
      "=== Epoch: 1, Train Loss: 758.0403942465782\n",
      "=== Epoch: 2, Train Loss: 752.8756191730499\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "train_each_iteration_loss = list()\n",
    "train_loss_list = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                     user_occupation_feature=torch_user_occupation,\n",
    "                     movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result)\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 10\n",
    "\n",
    "# 1. 把所有東西包成dataloader\n",
    "train_dataset = TensorDataset( torch.FloatTensor(train_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(train_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(train_movie_feature_data.values),\n",
    "                               torch.FloatTensor(train_result_data) )\n",
    "test_dataset = TensorDataset(  torch.FloatTensor(test_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(test_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(test_movie_feature_data.values),\n",
    "                               torch.FloatTensor(test_result_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "# 2. 呼叫模型與設定Loss function\n",
    "model = ipnn_model(num_user_age=train_user_age_onehotencoding.shape[1], \n",
    "                 num_user_occupation=train_user_occupation_onehotencoding.shape[1], \n",
    "                 num_movie_genre=train_movie_feature_data.values.shape[1],\n",
    "                 num_decoder=num_features*3+3,\n",
    "                 num_features=num_features)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0, Train Loss: 2269.737111568451\n",
      "=== Epoch: 1, Train Loss: 744.1373466849327\n",
      "=== Epoch: 2, Train Loss: 743.6776596307755\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "train_each_iteration_loss = list()\n",
    "train_loss_list = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                     user_occupation_feature=torch_user_occupation,\n",
    "                     movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result)\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDBT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "338435f3ace29f4c96579772b339886e16b2e32c5ce1705983594e7aef710dc8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
