{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import all_model_for_movielens as movielens_model\n",
    "import all_model_for_douban_book as book_model\n",
    "from User_based_CF import *\n",
    "from Item_based_CF import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix(data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    data: (user_column_name, item_column_name, result_name, timestamp)\n",
    "    \"\"\"\n",
    "    user_list = data.iloc[:, 0]\n",
    "    item_list = data.iloc[:, 1]\n",
    "    rating_list = data[result_name].values\n",
    "    user_item_matrix_data = pd.crosstab(index=user_list, columns=item_list, values=rating_list, aggfunc=np.mean)\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify whether the value exists or not.\n",
    "def identify_value_exist(user_item_matrix_data):\n",
    "    \"\"\"\n",
    "    user_item_matrix_data: DataFrame\n",
    "    \"\"\"\n",
    "    return (user_item_matrix_data.isna() == False).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "def create_user_item_matrix_for_matrix_factorization(data, unique_user_id, unique_item_id):\n",
    "    \"\"\"\n",
    "    data: (user, item, rating, timestamp)\n",
    "    \"\"\"\n",
    "    user_item_matrix_data = pd.DataFrame(np.array([np.nan] * (len(unique_user_id) * len(unique_item_id))).reshape(len(unique_user_id), len(unique_item_id)),\\\n",
    "        index=unique_user_id, columns=unique_item_id)\n",
    "    \n",
    "    for one_index in data.index:\n",
    "        user_item_matrix_data.loc[data.loc[one_index, \"User_id\"], data.loc[one_index, \"Item_id\"]] = \\\n",
    "            data.loc[one_index, \"Rating\"]\n",
    "    return user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要建構四種資料，分別為User的特徵、Item的特徵、User-Item matrix與User對應Item的紀錄\n",
    "def split_four_data(user_data, item_data, user_item_interaction_data, user_column_name, item_column_name, result_name):\n",
    "    \"\"\"\n",
    "    user_data：使用者相關資料（user_id一定要放第一個column）\n",
    "    item_data：物品相關資料（item_id一定要放第二個column）\n",
    "    \"\"\"\n",
    "    all_data = list()\n",
    "    if isinstance(user_data, pd.DataFrame):\n",
    "        # user_feature_data = user_data.iloc[:, 1:]\n",
    "        all_data.append(user_data)\n",
    "    \n",
    "    if isinstance(item_data, pd.DataFrame):\n",
    "        # item_feature_data = item_data.iloc[:, 1:]\n",
    "        all_data.append(item_data)\n",
    "    \n",
    "    if isinstance(user_item_interaction_data, pd.DataFrame):\n",
    "        # transform train data into user-item matrix\n",
    "        user_item_matrix_data = create_user_item_matrix(user_item_interaction_data, user_column_name, item_column_name, result_name)\n",
    "        all_data.append(user_item_interaction_data)\n",
    "        all_data.append(user_item_matrix_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義OneHotEncoding的內容→Movielens\n",
    "def movielens_onehotencoding(**data):\n",
    "    user_id_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"user_id\"].values.reshape((-1, 1)))\n",
    "    movie_id_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"movie_id\"].values.reshape((-1, 1)))\n",
    "    user_age_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"user_age\"].values.reshape((-1, 1)))\n",
    "    user_occupation_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"user_occupation\"].values.reshape((-1, 1)))\n",
    "    return user_id_onehotencoding, movie_id_onehotencoding, user_age_onehotencoding, user_occupation_onehotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 建立OneHotEncoder\n",
    "# 定義OneHotEncoding的內容→Movielens\n",
    "def book_onehotencoding(merge_data):\n",
    "    book_user_id_onehotencoding = OneHotEncoder(sparse=False).fit(merge_data[\"user_id\"].values.reshape((-1, 1)))\n",
    "    book_book_id_onehotencoding = OneHotEncoder(sparse=False).fit(merge_data[\"book_id\"].values.reshape((-1, 1)))\n",
    "    book_book_author_onehotencoding = OneHotEncoder(sparse=False).fit(merge_data[\"author\"].values.reshape((-1, 1)))\n",
    "    book_book_publisher_onehotencoding = OneHotEncoder(sparse=False).fit(merge_data[\"publisher\"].values.reshape((-1, 1)))\n",
    "    book_book_year_onehotencoding = OneHotEncoder(sparse=False).fit(merge_data[\"year\"].values.reshape((-1, 1)))\n",
    "    return book_user_id_onehotencoding, book_book_id_onehotencoding, book_book_author_onehotencoding, book_book_publisher_onehotencoding, book_book_year_onehotencoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將每種不同資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\Movielens\\movie_genre.dat\", \"r\") as f:\n",
    "    movie_genre = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_genre = pd.DataFrame(np.array(movie_genre), columns=[\"movie_id\", \"genre\"])\n",
    "movie_genre[\"genre\"] = movie_genre[\"genre\"].astype(\"str\")\n",
    "\n",
    "with open(r\"data\\Movielens\\movie_movie(knn).dat\", \"r\") as f:\n",
    "    movie_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "movie_movie = pd.DataFrame(np.array(movie_movie), columns=[\"movie1\", \"movie2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_age.dat\", \"r\") as f:\n",
    "    user_age = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_age = pd.DataFrame(np.array(user_age), columns=[\"user_id\", \"age\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_occupation.dat\", \"r\") as f:\n",
    "    user_occupation = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_occupation = pd.DataFrame(np.array(user_occupation), columns=[\"user_id\", \"occupation\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_user(knn).dat\", \"r\") as f:\n",
    "    user_user = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_user = pd.DataFrame(np.array(user_user), columns=[\"user1\", \"user2\", \"similarity\"])\n",
    "\n",
    "with open(r\"data\\Movielens\\user_movie.dat\", \"r\") as f:\n",
    "    user_movie = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_movie = pd.DataFrame(np.array(user_movie), columns=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"])\n",
    "user_movie[\"rating\"] = user_movie[\"rating\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build User-Item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix_data = create_user_item_matrix(user_movie, user_column_name=\"user_id\", item_column_name=\"item_id\", result_name=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義OneHotEncoding的內容→Movielens\n",
    "movielens_rating_user_id_onehotencoding, movielens_rating_movie_id_onehotencoding, movielens_rating_user_age_onehotencoding, movielens_rating_user_occupation_onehotencoding =\\\n",
    "    movielens_onehotencoding(**{\"user_id\":user_movie[\"user_id\"], \"movie_id\":user_movie[\"movie_id\"], \"user_age\":user_age[\"age\"], \"user_occupation\":user_occupation[\"occupation\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 針對電影種類前處理：由於一部電影可能有多種種類，因此將每個種類用OneHotEncoding表示\n",
    "movie_genre[\"index\"] = 1\n",
    "movie_genre = movie_genre.pivot_table(index=\"movie_id\", columns=\"genre\", values=\"index\", fill_value=0)\n",
    "movie_genre = movie_genre.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把原始資料定義為輸入資料的過程（包含訓練與測試資料）\n",
    "# 最終想生成哪些資料\n",
    "# 1. 訓練資料、測試資料\n",
    "# 2. 每個資料的單純User-id、Item-id, User-id所有特徵、Item-id所有特徵、User-Item-interaction、User-Item Matrix\n",
    "    # 把資料分割成訓練資料與測試資料\n",
    "def generate_input_data_movielens(rating_train_merge_data, rating_test_merge_data, target):\n",
    "    rating_train_user_id_data, rating_test_user_id_data = rating_train_merge_data[\"user_id\"], rating_test_merge_data[\"user_id\"] # 目標\n",
    "    rating_train_movie_id_data, rating_test_movie_id_data = rating_train_merge_data[\"movie_id\"], rating_test_merge_data[\"movie_id\"] # 目標\n",
    "    rating_train_user_feature_data, rating_test_user_feature_data = rating_train_merge_data[[\"age\", \"occupation\"]], rating_test_merge_data[[\"age\", \"occupation\"]]\n",
    "    rating_train_movie_feature_data, rating_test_movie_feature_data = rating_train_merge_data[movie_genre.columns[1:]], rating_test_merge_data[movie_genre.columns[1:]]\n",
    "    rating_train_result_data, rating_test_result_data = rating_train_merge_data[target].values, rating_test_merge_data[target].values\n",
    "\n",
    "    try:\n",
    "        rating_train_user_feature_data, rating_train_movie_feature_data, rating_train_user_item_interaction_data, rating_train_user_item_matrix_data =\\\n",
    "            split_four_data(rating_train_user_feature_data, rating_train_movie_feature_data, rating_train_merge_data[[\"user_id\", \"movie_id\", target, \"timestamp\"]], user_column_name=\"user_id\", item_column_name=\"movie_id\", result_name=target)\n",
    "\n",
    "        rating_test_user_feature_data, rating_test_movie_feature_data, rating_test_user_item_interaction_data, rating_test_user_item_matrix_data =\\\n",
    "            split_four_data(rating_test_user_feature_data, rating_test_movie_feature_data, rating_test_merge_data[[\"user_id\", \"movie_id\", target, \"timestamp\"]], user_column_name=\"user_id\", item_column_name=\"movie_id\", result_name=target)\n",
    "    except:\n",
    "        rating_train_user_feature_data, rating_train_movie_feature_data, rating_train_user_item_interaction_data, rating_train_user_item_matrix_data =\\\n",
    "            split_four_data(rating_train_user_feature_data, rating_train_movie_feature_data, rating_train_merge_data[[\"user_id\", \"movie_id\", target]], user_column_name=\"user_id\", item_column_name=\"movie_id\", result_name=target)\n",
    "\n",
    "        rating_test_user_feature_data, rating_test_movie_feature_data, rating_test_user_item_interaction_data, rating_test_user_item_matrix_data =\\\n",
    "            split_four_data(rating_test_user_feature_data, rating_test_movie_feature_data, rating_test_merge_data[[\"user_id\", \"movie_id\", target]], user_column_name=\"user_id\", item_column_name=\"movie_id\", result_name=target)\n",
    "\n",
    "\n",
    "    # 2. 把所有訓練資料以及測試資料都轉成OneHotEncoding\n",
    "    rating_train_user_age_onehotencoding, rating_test_user_age_onehotencoding =\\\n",
    "        list(map(lambda x: movielens_rating_user_age_onehotencoding.transform(x), [rating_train_user_feature_data[\"age\"].values.reshape((-1, 1)), rating_test_user_feature_data[\"age\"].values.reshape((-1, 1))]))\n",
    "    rating_train_user_occupation_onehotencoding, rating_test_user_occupation_onehotencoding =\\\n",
    "        list(map(lambda x: movielens_rating_user_occupation_onehotencoding.transform(x), [rating_train_user_feature_data[\"occupation\"].values.reshape((-1, 1)), rating_test_user_feature_data[\"occupation\"].values.reshape((-1, 1))]))\n",
    "    rating_train_user_id_onehotencoding, rating_test_user_id_onehotencoding =\\\n",
    "        list(map(lambda x: movielens_rating_user_id_onehotencoding.transform(x), [rating_train_user_id_data.values.reshape((-1, 1)), rating_test_user_id_data.values.reshape((-1, 1))]))\n",
    "    rating_train_movie_id_onehotencoding, rating_test_movie_id_onehotencoding =\\\n",
    "        list(map(lambda x: movielens_rating_movie_id_onehotencoding.transform(x), [rating_train_movie_id_data.values.reshape((-1, 1)), rating_test_movie_id_data.values.reshape((-1, 1))]))\n",
    "    \n",
    "    return rating_train_user_id_onehotencoding, rating_test_user_id_onehotencoding,\\\n",
    "           rating_train_movie_id_onehotencoding, rating_test_movie_id_onehotencoding,\\\n",
    "           rating_train_user_age_onehotencoding, rating_test_user_age_onehotencoding,\\\n",
    "           rating_train_user_occupation_onehotencoding, rating_test_user_occupation_onehotencoding,\\\n",
    "           rating_train_movie_feature_data, rating_test_movie_feature_data,\\\n",
    "           rating_train_result_data, rating_test_result_data,\\\n",
    "           rating_train_user_item_interaction_data, rating_test_user_item_interaction_data,\\\n",
    "           rating_train_user_item_matrix_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "綜合上述前處理過程，只需要保留以下幾個變數：\n",
    "1. rating_train_user_id_onehoteocoding, rating_test_user_id_onehotencoding\n",
    "2. rating_train_movie_id_onehotencoding, rating_test_movie_id_onehotencoding\n",
    "3. rating_train_user_age_onehotencoding, rating_test_user_age_onehotencoding\n",
    "4. rating_train_user_occupation_onehotencoding, rating_test_user_occupation_onehotencoding\n",
    "5. rating_train_movie_feature_data, rating_test_movie_feature_data\n",
    "6. rating_train_result_data, rating_test_result_data\n",
    "7. rating_train_user_item_interaction_data, rating_test_user_item_interaction_data: pd.DataFrame\n",
    "8. rating_train_user_item_matrix_data: pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n",
      "(100000, 6)\n",
      "(100000, 24)\n"
     ]
    }
   ],
   "source": [
    "movielens_rating_merge_data = pd.merge(user_movie, user_age, how=\"inner\", on=\"user_id\")\n",
    "print(movielens_rating_merge_data.shape)\n",
    "movielens_rating_merge_data = pd.merge(movielens_rating_merge_data, user_occupation, how=\"left\", on=\"user_id\")\n",
    "print(movielens_rating_merge_data.shape)\n",
    "movielens_rating_merge_data = pd.merge(movielens_rating_merge_data, movie_genre, how=\"left\", on=\"movie_id\").fillna(0)\n",
    "print(movielens_rating_merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要Cross-Validation，所以需要分成五個不同的index組合\n",
    "# 1. 定義出所有的index\n",
    "movielens_rating_merge_data_index = movielens_rating_merge_data.index.values\n",
    "\n",
    "# 2. 隨機打亂\n",
    "np.random.seed(12345)\n",
    "np.random.shuffle(movielens_rating_merge_data_index)\n",
    "\n",
    "# 3. 開始分組\n",
    "threshold = int(round(len(movielens_rating_merge_data_index)/5, 0))\n",
    "movielens_rating_index_group = list(map(lambda x: list(movielens_rating_merge_data_index[x * threshold:-((5-(x+1))*2+1)]), list(range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生binary data→定義某個user是否會對某個item做評論\n",
    "# 1. 產生某個值是否為真實值\n",
    "movielens_identify_value_exist_binary = (user_item_matrix_data.isna() == False).astype(\"int\")\n",
    "\n",
    "# 2. 產生index名稱為column\n",
    "movielens_identify_value_exist_binary[\"user_id\"] = movielens_identify_value_exist_binary.index\n",
    "\n",
    "# 3. pandas melt\n",
    "movielens_binary_result = pd.melt(movielens_identify_value_exist_binary, id_vars=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586126, 4)\n",
      "(1586126, 5)\n",
      "(1586126, 23)\n"
     ]
    }
   ],
   "source": [
    "movielens_binary_merge_data = pd.merge(movielens_binary_result, user_age, how=\"inner\", on=\"user_id\")\n",
    "print(movielens_binary_merge_data.shape)\n",
    "movielens_binary_merge_data = pd.merge(movielens_binary_merge_data, user_occupation, how=\"left\", on=\"user_id\")\n",
    "print(movielens_binary_merge_data.shape)\n",
    "movielens_binary_merge_data = pd.merge(movielens_binary_merge_data, movie_genre, how=\"left\", on=\"movie_id\").fillna(0)\n",
    "print(movielens_binary_merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要Cross-Validation，所以需要分成五個不同的index組合\n",
    "# 1. 定義出所有的index\n",
    "movielens_binary_merge_data_index = movielens_binary_merge_data.index.values\n",
    "\n",
    "# 2. 隨機打亂\n",
    "np.random.seed(12345)\n",
    "np.random.shuffle(movielens_binary_merge_data_index)\n",
    "\n",
    "# 3. 開始分組\n",
    "threshold = int(round(len(movielens_binary_merge_data_index)/5, 0))\n",
    "movielens_binary_index_group = list(map(lambda x: list(movielens_binary_merge_data_index[x * threshold:-((5-(x+1))*2+1)]), list(range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Douban_Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\Douban_Book\\book_author.dat\", \"r\") as f:\n",
    "    book_author = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "book_author = pd.DataFrame(np.array(book_author), columns=[\"book_id\", \"author\"])\n",
    "\n",
    "with open(r\"data\\Douban_Book\\book_publisher.dat\", \"r\") as f:\n",
    "    book_publisher = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "book_publisher = pd.DataFrame(np.array(book_publisher), columns=[\"book_id\", \"publisher\"])\n",
    "\n",
    "with open(r\"data\\Douban_Book\\book_year.dat\", \"r\") as f:\n",
    "    book_year = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "book_year = pd.DataFrame(np.array(book_year), columns=[\"book_id\", \"year\"])\n",
    "\n",
    "with open(r\"data\\Douban_Book\\user_group.dat\", \"r\") as f:\n",
    "    user_group = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_group = pd.DataFrame(np.array(user_group), columns=[\"user_id\", \"group\"])\n",
    "\n",
    "with open(r\"data\\Douban_Book\\user_book.dat\", \"r\") as f:\n",
    "    user_book = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_book = pd.DataFrame(np.array(user_book), columns=[\"user_id\", \"book_id\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 針對user的group進行前處理→由於一個user可能有多個group，因此先對user的group進行One-Hot Encoding\n",
    "user_group[\"index\"] = 1\n",
    "user_group = user_group.pivot_table(index=\"user_id\", columns=\"group\", values=\"index\", fill_value=0)\n",
    "user_group = user_group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792062, 4)\n",
      "(792062, 5)\n",
      "(792062, 6)\n",
      "(792062, 2942)\n"
     ]
    }
   ],
   "source": [
    "# 2. merge data\n",
    "book_rating_merge_data = pd.merge(user_book, book_author, how=\"left\", on=\"book_id\")\n",
    "print(book_rating_merge_data.shape)\n",
    "book_rating_merge_data = pd.merge(book_rating_merge_data, book_publisher, how=\"left\", on=\"book_id\")\n",
    "print(book_rating_merge_data.shape)\n",
    "book_rating_merge_data = pd.merge(book_rating_merge_data, book_year, how=\"left\", on=\"book_id\")\n",
    "print(book_rating_merge_data.shape)\n",
    "book_rating_merge_data = pd.merge(book_rating_merge_data, user_group, how=\"left\", on=\"user_id\")\n",
    "print(book_rating_merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立OneHotEncoding\n",
    "book_user_id_onehotencoding, book_book_id_onehotencoding, book_book_author_onehotencoding, book_book_publisher_onehotencoding, book_book_year_onehotencoding =\\\n",
    "    book_onehotencoding(book_rating_merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data_douban_book(book_rating_train_merge_data, book_rating_test_merge_data, target):\n",
    "    book_rating_train_user_id_data, book_rating_test_user_id_data = book_rating_train_merge_data[\"user_id\"], book_rating_test_merge_data[\"user_id\"] # 目標\n",
    "    book_rating_train_book_id_data, book_rating_test_book_id_data = book_rating_train_merge_data[\"book_id\"], book_rating_test_merge_data[\"book_id\"] # 目標\n",
    "    book_rating_train_user_feature_data, book_rating_test_user_feature_data = book_rating_train_merge_data[user_group.columns[1:]], book_rating_test_merge_data[user_group.columns[1:]]\n",
    "    book_rating_train_book_feature_data, book_rating_test_book_feature_data = book_rating_train_merge_data[[\"author\", \"publisher\", \"year\"]], book_rating_test_merge_data[[\"author\", \"publisher\", \"year\"]]\n",
    "    book_rating_train_result_data, book_rating_test_result_data = book_rating_train_merge_data[target].values, book_rating_test_merge_data[target].values\n",
    "\n",
    "    book_train_user_item_matrix_data = pd.crosstab(index=book_rating_train_merge_data[\"user_id\"], \n",
    "                                             columns=book_rating_train_merge_data[\"book_id\"],\n",
    "                                             values=book_rating_train_merge_data[target],\n",
    "                                             aggfunc=np.mean)\n",
    "\n",
    "    # 2. 把所有訓練資料以及測試資料都轉成OneHotEncoding\n",
    "    book_rating_train_book_author_onehotencoding, book_rating_test_book_author_onehotencoding =\\\n",
    "        list(map(lambda x: book_book_author_onehotencoding.transform(x), [book_rating_train_book_feature_data[\"author\"].values.reshape((-1, 1)), book_rating_test_book_feature_data[\"author\"].values.reshape((-1, 1))]))\n",
    "    book_rating_train_book_publisher_onehotencoding, book_rating_test_book_publisher_onehotencoding =\\\n",
    "        list(map(lambda x: book_book_publisher_onehotencoding.transform(x), [book_rating_train_book_feature_data[\"publisher\"].values.reshape((-1, 1)), book_rating_test_book_feature_data[\"publisher\"].values.reshape((-1, 1))]))\n",
    "    book_rating_train_book_year_onehotencoding, book_rating_test_book_year_onehotencoding =\\\n",
    "        list(map(lambda x: book_book_year_onehotencoding.transform(x), [book_rating_train_book_feature_data[\"year\"].values.reshape((-1, 1)), book_rating_test_book_feature_data[\"year\"].values.reshape((-1, 1))]))\n",
    "    \n",
    "    book_rating_train_user_id_onehotencoding, book_rating_test_user_id_onehotencoding =\\\n",
    "        list(map(lambda x: book_user_id_onehotencoding.transform(x), [book_rating_train_user_id_data.values.reshape((-1, 1)), book_rating_test_user_id_data.values.reshape((-1, 1))]))\n",
    "    book_rating_train_book_id_onehotencoding, book_rating_test_book_id_onehotencoding =\\\n",
    "        list(map(lambda x: book_book_id_onehotencoding.transform(x), [book_rating_train_book_id_data.values.reshape((-1, 1)), book_rating_test_book_id_data.values.reshape((-1, 1))]))\n",
    "    \n",
    "    return book_rating_train_user_id_onehotencoding, book_rating_test_user_id_onehotencoding,\\\n",
    "           book_rating_train_book_id_onehotencoding, book_rating_test_book_id_onehotencoding,\\\n",
    "           book_rating_train_book_author_onehotencoding, book_rating_test_book_author_onehotencoding,\\\n",
    "           book_rating_train_book_publisher_onehotencoding, book_rating_test_book_publisher_onehotencoding,\\\n",
    "           book_rating_train_book_year_onehotencoding, book_rating_test_book_year_onehotencoding,\\\n",
    "           book_rating_train_user_feature_data, book_rating_test_user_feature_data,\\\n",
    "           book_rating_train_result_data, book_rating_test_result_data,\\\n",
    "           book_rating_train_merge_data[[\"user_id\", \"book_id\", target]], book_rating_test_merge_data[[\"user_id\", \"book_id\", target]],\\\n",
    "           book_train_user_item_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. split 5 groups test data\n",
    "\n",
    "# 1. 定義出所有的index\n",
    "book_rating_merge_data_index = book_rating_merge_data.index.values\n",
    "\n",
    "# 2. 隨機打亂\n",
    "np.random.seed(12345)\n",
    "np.random.shuffle(book_rating_merge_data_index)\n",
    "\n",
    "# 3. 開始分組\n",
    "threshold = int(round(len(book_rating_merge_data_index)/5, 0))\n",
    "book_rating_index_group = list(map(lambda x: list(book_rating_merge_data_index[x * threshold:-((5-(x+1))*2+1)]), list(range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\Yelp\\business_category.dat\", \"r\") as f:\n",
    "    business_category = [i.replace(\"\\n\", \"\").split(\"\\t\")[:2] for i in f.readlines()]\n",
    "business_category = pd.DataFrame(np.array(business_category), columns=[\"business_id\", \"category\"])\n",
    "\n",
    "with open(r\"data\\Yelp\\business_city.dat\", \"r\") as f:\n",
    "    business_city = [i.replace(\"\\n\", \"\").split(\"\\t\")[:2] for i in f.readlines()]\n",
    "business_city = pd.DataFrame(np.array(business_city), columns=[\"business_id\", \"city\"])\n",
    "\n",
    "with open(r\"data\\Yelp\\user_business.dat\", \"r\") as f:\n",
    "    user_business = [i.replace(\"\\n\", \"\").split(\"\\t\") for i in f.readlines()]\n",
    "user_business = pd.DataFrame(np.array(user_business), columns=[\"user_id\", \"business_id\", \"rating\"])\n",
    "\n",
    "with open(r\"data\\Yelp\\user_compliment.dat\", \"r\") as f:\n",
    "    user_compliment = [i.replace(\"\\n\", \"\").split(\"\\t\")[:2] for i in f.readlines()]\n",
    "user_compliment = pd.DataFrame(np.array(user_compliment), columns=[\"user_id\", \"compliment\"])\n",
    "\n",
    "with open(r\"data\\Yelp\\user_user.dat\", \"r\") as f:\n",
    "    user_user = [i.replace(\"\\n\", \"\").split(\"\\t\")[:2] for i in f.readlines()]\n",
    "user_user = pd.DataFrame(np.array(user_user), columns=[\"user1\", \"user2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build User-Business Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_matrix = pd.crosstab(index=user_business[\"user_id\"], columns=user_business[\"business_id\"], values=user_business[\"rating\"], aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_merge_data = pd.concat([user_business, user_compliment], on=\"user_id\", how=\"left\")\n",
    "print(yelp_merge_data.shape)\n",
    "yelp_merge_data = pd.concat([yelp_merge_data, business_city], on=\"business_id\", how=\"left\")\n",
    "print(yelp_merge_data.shape)\n",
    "yelp_merge_data = pd.concat([yelp_merge_data, business_category], one=\"business_id\", how=\"left\")\n",
    "print(yelp_merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_onehotencoding(**data):\n",
    "    yelp_user_compliment_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"compliment\"])\n",
    "    yelp_user_id_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"user_id\"])\n",
    "    yelp_business_id_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"business_id\"])\n",
    "    yelp_business_category_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"category\"])\n",
    "    yelp_business_city_onehotencoding = OneHotEncoder(sparse=False).fit(data[\"city\"])\n",
    "    return yelp_user_id_onehotencoding, yelp_business_id_onehotencoding, yelp_user_compliment_onehotencoding, yelp_business_city_onehotencoding, yelp_business_category_onehotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data_yelp(yelp_train_merge_data, yelp_test_merge_data):\n",
    "    yelp_rating_train_user_id, yelp_rating_test_user_id = yelp_train_merge_data[\"user_id\"], yelp_test_merge_data[\"user_id\"]\n",
    "    yelp_rating_train_business_id, yelp_rating_test_business_id = yelp_train_merge_data[\"business_id\"], yelp_test_merge_data[\"business_id\"]\n",
    "    yelp_rating_train_user_compliment, yelp_rating_test_user_compliment = yelp_train_merge_data[\"compliment\"], yelp_test_merge_data[\"compliment\"]\n",
    "    yelp_rating_train_business_city, yelp_rating_test_business_city = yelp_train_merge_data[\"city\"], yelp_test_merge_data[\"city\"]\n",
    "    yelp_rating_train_business_category, yelp_rating_test_business_category = yelp_train_merge_data[\"category\"], yelp_test_merge_data[\"category\"]\n",
    "\n",
    "    yelp_user_business_matrix_data = pd.crosstab(index=yelp_train_merge_data[\"user_id\"], columns=yelp_train_merge_data[\"business_id\"], values=yelp_train_merge_data[\"rating\"], aggfunc=np.mean)\n",
    "\n",
    "    yelp_rating_train_user_id_onehotencoding, yelp_rating_test_user_id_onehotencoding = \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 889249/889249 [04:54<00:00, 3018.10it/s]\n",
      "100%|██████████| 99991/99991 [01:54<00:00, 871.45it/s]\n",
      "100%|██████████| 2829124/2829124 [05:11<00:00, 9096.13it/s]\n",
      "100%|██████████| 99991/99991 [02:13<00:00, 750.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# [\"Model\", \"Methods\", \"K\", \"RMSE\", \"fitted_train_time\", \"predict_time\"]：預計報表會有的欄位\n",
    "similarity_method = [\"pearson\", \"cosine\"]\n",
    "K_list = [3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "CF_totalresult_list = list()\n",
    "\n",
    "for one_group in list(range(5)):\n",
    "    # 生成前處理後的輸入資料\n",
    "    rating_train_user_id_onehotencoding, rating_test_user_id_onehotencoding,\\\n",
    "    rating_train_movie_id_onehotencoding, rating_test_movie_id_onehotencoding,\\\n",
    "    rating_train_user_age_onehotencoding, rating_test_user_age_onehotencoding,\\\n",
    "    rating_train_user_occupation_onehotencoding, rating_test_user_occupation_onehotencoding,\\\n",
    "    rating_train_movie_feature_data, rating_test_movie_feature_data,\\\n",
    "    rating_train_result_data, rating_test_result_data,\\\n",
    "    rating_train_user_item_interaction_data, rating_test_user_item_interaction_data,\\\n",
    "    rating_train_user_item_matrix_data = generate_input_data_movielens(rating_train_merge_data=rating_merge_data.drop(index=rating_index_group[one_group]), \n",
    "                                                                       rating_test_merge_data=rating_merge_data.loc[rating_index_group[one_group], :])\n",
    "\n",
    "    for one_similarity_method in similarity_method:\n",
    "        # User-based Collaborative Filtering\n",
    "        start_time = time.time()\n",
    "        user_cf = User_based_CF(rating_train_user_item_interaction_data, user_item_matrix_data)\n",
    "        user_user_correlation_data = user_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        for K in K_list:\n",
    "            # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "            start_time = time.time()\n",
    "            pred_user_data = list(map(lambda x: user_cf.predict_without_time(rating_test_user_item_interaction_data.iloc[x, 0], rating_test_user_item_interaction_data.iloc[x, 1], user_column_name=\"user_id\", item_column_name=\"movie_id\", num_user=K), tqdm([i for i in range(rating_test_user_item_interaction_data.shape[0])])))\n",
    "            pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "            end_time = time.time()\n",
    "            predict_time = end_time - start_time\n",
    "            rmse_loss = math.sqrt(mean_squared_error(y_true=rating_test_user_item_interaction_data[\"rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "            CF_totalresult_list.append([\"User_CF\", f\"group_{one_group}\", one_similarity_method, K, rmse_loss, train_time, predict_time])\n",
    "            writer = pd.ExcelWriter(\"report/CF_model_result.xlsx\")\n",
    "            pd.DataFrame(np.array(CF_totalresult_list), columns=[\"Model\", \"test_group\", \"methods\", \"K\", \"RMSE\", \"fitted_time\", \"pred_time\"]).to_excel(writer, index=None)\n",
    "            writer.save()\n",
    "\n",
    "        # Item-based Collaborative Filtering\n",
    "        start_time = time.time()\n",
    "        item_cf = Item_based_CF(rating_train_user_item_interaction_data, user_item_matrix_data)\n",
    "        item_item_correlation_data = item_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        for K in K_list:\n",
    "            # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "            start_time = time.time()\n",
    "            pred_user_data = list(map(lambda x: item_cf.predict_without_time(rating_test_user_item_interaction_data.iloc[x, 0], rating_test_user_item_interaction_data.iloc[x, 1], user_column_name=\"user_id\", item_column_name=\"movie_id\", num_item=K), tqdm([i for i in range(rating_test_user_item_interaction_data.shape[0])])))\n",
    "            pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "            end_time = time.time()\n",
    "            predict_time = end_time - start_time\n",
    "            rmse_loss = math.sqrt(mean_squared_error(y_true=rating_test_user_item_interaction_data[\"rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "            CF_totalresult_list.append([\"Item_CF\", f\"group_{one_group}\", one_similarity_method, K, rmse_loss, train_time, predict_time])\n",
    "            writer = pd.ExcelWriter(\"report/CF_model_result.xlsx\")\n",
    "            pd.DataFrame(np.array(CF_totalresult_list), columns=[\"Model\", \"test_group\", \"methods\", \"K\", \"RMSE\", \"fitted_time\", \"pred_time\"]).to_excel(writer, index=None)\n",
    "            writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Douban Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"Model\", \"Methods\", \"K\", \"RMSE\", \"fitted_train_time\", \"predict_time\"]：預計報表會有的欄位\n",
    "similarity_method = [\"pearson\", \"cosine\"]\n",
    "K_list = [3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "CF_totalresult_list = list()\n",
    "\n",
    "for one_group in list(range(5)):\n",
    "    # 生成前處理後的輸入資料\n",
    "    book_rating_train_user_id_onehotencoding, book_rating_test_user_id_onehotencoding,\\\n",
    "    book_rating_train_book_id_onehotencoding, book_rating_test_book_id_onehotencoding,\\\n",
    "    book_rating_train_book_author_onehotencoding, book_rating_test_book_author_onehotencoding,\\\n",
    "    book_rating_train_book_publisher_onehotencoding, book_rating_test_book_publisher_onehotencoding,\\\n",
    "    book_rating_train_book_year_onehotencoding, book_rating_test_book_year_onehotencoding,\\\n",
    "    book_rating_train_user_feature_data, book_rating_test_user_feature_data,\\\n",
    "    book_rating_train_result_data, book_rating_test_result_data,\\\n",
    "    book_rating_train_user_item_interaction_data, book_rating_train_user_item_interaction_data,\\\n",
    "    book_train_user_item_matrix_data = generate_input_data_douban_book(book_rating_merge_data.drop(index=book_rating_index_group[one_group]), book_rating_merge_data.loc[book_rating_index_group[one_group], :])\n",
    "\n",
    "\n",
    "    for one_similarity_method in similarity_method:\n",
    "        # User-based Collaborative Filtering\n",
    "        start_time = time.time()\n",
    "        user_cf = User_based_CF(rating_train_user_item_interaction_data, user_item_matrix_data)\n",
    "        user_user_correlation_data = user_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        for K in K_list:\n",
    "            # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "            start_time = time.time()\n",
    "            pred_user_data = list(map(lambda x: user_cf.predict_without_time(rating_test_user_item_interaction_data.iloc[x, 0], rating_test_user_item_interaction_data.iloc[x, 1], user_column_name=\"user_id\", item_column_name=\"movie_id\", num_user=K), tqdm([i for i in range(rating_test_user_item_interaction_data.shape[0])])))\n",
    "            pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "            end_time = time.time()\n",
    "            predict_time = end_time - start_time\n",
    "            rmse_loss = math.sqrt(mean_squared_error(y_true=rating_test_user_item_interaction_data[\"rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "            CF_totalresult_list.append([\"User_CF\", f\"group_{one_group}\", one_similarity_method, K, rmse_loss, train_time, predict_time])\n",
    "            writer = pd.ExcelWriter(\"report/book_CF_model_result.xlsx\")\n",
    "            pd.DataFrame(np.array(CF_totalresult_list), columns=[\"Model\", \"test_group\", \"methods\", \"K\", \"RMSE\", \"fitted_time\", \"pred_time\"]).to_excel(writer, index=None)\n",
    "            writer.save()\n",
    "            break\n",
    "\n",
    "        # Item-based Collaborative Filtering\n",
    "        start_time = time.time()\n",
    "        item_cf = Item_based_CF(rating_train_user_item_interaction_data, user_item_matrix_data)\n",
    "        item_item_correlation_data = item_cf.compute_correlation(corr_methods=one_similarity_method)\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        for K in K_list:\n",
    "            # 針對test data做預測以及模型評估（注意，每次計算是針對一筆資料）\n",
    "            start_time = time.time()\n",
    "            pred_user_data = list(map(lambda x: item_cf.predict_without_time(rating_test_user_item_interaction_data.iloc[x, 0], rating_test_user_item_interaction_data.iloc[x, 1], user_column_name=\"user_id\", item_column_name=\"movie_id\", num_item=K), tqdm([i for i in range(rating_test_user_item_interaction_data.shape[0])])))\n",
    "            pred_user_data = [i if i > 0 else 0 for i in pred_user_data]\n",
    "            end_time = time.time()\n",
    "            predict_time = end_time - start_time\n",
    "            rmse_loss = math.sqrt(mean_squared_error(y_true=rating_test_user_item_interaction_data[\"rating\"].values, y_pred=np.array(pred_user_data)))\n",
    "            CF_totalresult_list.append([\"Item_CF\", f\"group_{one_group}\", one_similarity_method, K, rmse_loss, train_time, predict_time])\n",
    "            writer = pd.ExcelWriter(\"report/book_CF_model_result.xlsx\")\n",
    "            pd.DataFrame(np.array(CF_totalresult_list), columns=[\"Model\", \"test_group\", \"methods\", \"K\", \"RMSE\", \"fitted_time\", \"pred_time\"]).to_excel(writer, index=None)\n",
    "            writer.save()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 Train Loss: 20.599211405299222\n",
      "=== Epoch: 1 Train Loss: 20.59466254555195\n",
      "=== Epoch: 2 Train Loss: 20.590115948601476\n",
      "=== Epoch: 3 Train Loss: 20.585571626313254\n",
      "=== Epoch: 4 Train Loss: 20.581029547581313\n",
      "=== Epoch: 5 Train Loss: 20.57648974614625\n",
      "=== Epoch: 6 Train Loss: 20.57195217663575\n",
      "=== Epoch: 7 Train Loss: 20.567416873053997\n",
      "=== Epoch: 8 Train Loss: 20.56288380328726\n",
      "=== Epoch: 9 Train Loss: 20.55835299136292\n",
      "=== Epoch: 10 Train Loss: 20.553824444222204\n",
      "=== Epoch: 11 Train Loss: 20.549298140749443\n",
      "=== Epoch: 12 Train Loss: 20.544774087423903\n",
      "=== Epoch: 13 Train Loss: 20.54025230256187\n",
      "=== Epoch: 14 Train Loss: 20.535732735034546\n",
      "=== Epoch: 15 Train Loss: 20.531215408604808\n",
      "=== Epoch: 16 Train Loss: 20.526700349304868\n",
      "=== Epoch: 17 Train Loss: 20.522187512591717\n",
      "=== Epoch: 18 Train Loss: 20.51767692373556\n",
      "=== Epoch: 19 Train Loss: 20.513168575551603\n",
      "=== Epoch: 20 Train Loss: 20.50866248156334\n",
      "=== Epoch: 21 Train Loss: 20.50415860576128\n",
      "=== Epoch: 22 Train Loss: 20.4996569612431\n",
      "=== Epoch: 23 Train Loss: 20.495157562353914\n",
      "=== Epoch: 24 Train Loss: 20.49066039021396\n",
      "=== Epoch: 25 Train Loss: 20.486165466206646\n",
      "=== Epoch: 26 Train Loss: 20.48167279065365\n",
      "=== Epoch: 27 Train Loss: 20.477182322945104\n",
      "=== Epoch: 28 Train Loss: 20.472694078446747\n",
      "=== Epoch: 29 Train Loss: 20.468208070293315\n",
      "=== Epoch: 30 Train Loss: 20.463724284641156\n",
      "=== Epoch: 31 Train Loss: 20.459242725947608\n",
      "=== Epoch: 32 Train Loss: 20.45476338660196\n",
      "=== Epoch: 33 Train Loss: 20.450286285978027\n",
      "=== Epoch: 34 Train Loss: 20.44581139180416\n",
      "=== Epoch: 35 Train Loss: 20.441338699093993\n",
      "=== Epoch: 36 Train Loss: 20.43686825580392\n",
      "=== Epoch: 37 Train Loss: 20.432400035400256\n",
      "=== Epoch: 38 Train Loss: 20.427934005322353\n",
      "=== Epoch: 39 Train Loss: 20.42347021203003\n",
      "=== Epoch: 40 Train Loss: 20.419008631589737\n",
      "=== Epoch: 41 Train Loss: 20.4145492521406\n",
      "=== Epoch: 42 Train Loss: 20.41009209023775\n",
      "=== Epoch: 43 Train Loss: 20.40563713246664\n",
      "=== Epoch: 44 Train Loss: 20.401184405557274\n",
      "=== Epoch: 45 Train Loss: 20.396733879410558\n",
      "=== Epoch: 46 Train Loss: 20.392285536676933\n",
      "=== Epoch: 47 Train Loss: 20.387839399240793\n",
      "=== Epoch: 48 Train Loss: 20.38339550024515\n",
      "=== Epoch: 49 Train Loss: 20.378953813715643\n",
      "=== Epoch: 50 Train Loss: 20.374514300018543\n",
      "=== Epoch: 51 Train Loss: 20.370076973033676\n",
      "=== Epoch: 52 Train Loss: 20.36564186045224\n",
      "=== Epoch: 53 Train Loss: 20.361208964234347\n",
      "=== Epoch: 54 Train Loss: 20.356778253037742\n",
      "=== Epoch: 55 Train Loss: 20.35234974528683\n",
      "=== Epoch: 56 Train Loss: 20.347923438736363\n",
      "=== Epoch: 57 Train Loss: 20.343499322544393\n",
      "=== Epoch: 58 Train Loss: 20.339077382183234\n",
      "=== Epoch: 59 Train Loss: 20.334657632960973\n",
      "=== Epoch: 60 Train Loss: 20.330240091173053\n",
      "=== Epoch: 61 Train Loss: 20.32582475381487\n",
      "=== Epoch: 62 Train Loss: 20.321411597993887\n",
      "=== Epoch: 63 Train Loss: 20.317000590602667\n",
      "=== Epoch: 64 Train Loss: 20.312591779568045\n",
      "=== Epoch: 65 Train Loss: 20.30818519733578\n",
      "=== Epoch: 66 Train Loss: 20.30378077937936\n",
      "=== Epoch: 67 Train Loss: 20.299378530816895\n",
      "=== Epoch: 68 Train Loss: 20.2949784737708\n",
      "=== Epoch: 69 Train Loss: 20.290580585169256\n",
      "=== Epoch: 70 Train Loss: 20.286184895397398\n",
      "=== Epoch: 71 Train Loss: 20.28179137695623\n",
      "=== Epoch: 72 Train Loss: 20.27740002977915\n",
      "=== Epoch: 73 Train Loss: 20.27301086719819\n",
      "=== Epoch: 74 Train Loss: 20.268623857940312\n",
      "=== Epoch: 75 Train Loss: 20.26423902514937\n",
      "=== Epoch: 76 Train Loss: 20.259856394159527\n",
      "=== Epoch: 77 Train Loss: 20.255475927698587\n",
      "=== Epoch: 78 Train Loss: 20.25109761722525\n",
      "=== Epoch: 79 Train Loss: 20.24672145334596\n",
      "=== Epoch: 80 Train Loss: 20.242347476875523\n",
      "=== Epoch: 81 Train Loss: 20.237975660390145\n",
      "=== Epoch: 82 Train Loss: 20.233606028708937\n",
      "=== Epoch: 83 Train Loss: 20.22923855172923\n",
      "=== Epoch: 84 Train Loss: 20.22487324903178\n",
      "=== Epoch: 85 Train Loss: 20.22051007300192\n",
      "=== Epoch: 86 Train Loss: 20.216149085621414\n",
      "=== Epoch: 87 Train Loss: 20.211790245314212\n",
      "=== Epoch: 88 Train Loss: 20.207433560316396\n",
      "=== Epoch: 89 Train Loss: 20.20307904371354\n",
      "=== Epoch: 90 Train Loss: 20.19872668141008\n",
      "=== Epoch: 91 Train Loss: 20.194376467451576\n",
      "=== Epoch: 92 Train Loss: 20.190028406257046\n",
      "=== Epoch: 93 Train Loss: 20.185682505095283\n",
      "=== Epoch: 94 Train Loss: 20.18133873847724\n",
      "=== Epoch: 95 Train Loss: 20.17699714914151\n",
      "=== Epoch: 96 Train Loss: 20.17265769389493\n",
      "=== Epoch: 97 Train Loss: 20.16832039316556\n",
      "=== Epoch: 98 Train Loss: 20.163985233349702\n",
      "=== Epoch: 99 Train Loss: 20.159652229270964\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 1e-2\n",
    "num_user_id = user_item_matrix_data.shape[0]\n",
    "num_item_id = user_item_matrix_data.shape[1]\n",
    "num_features = 10\n",
    "\n",
    "model = matrix_factorization(true_user_item_matrix=rating_train_user_item_matrix_data, num_features=num_features)\n",
    "model.fit(epochs=epochs, learning_rate=learning_rate, regularization_rate=1e-2, bias_or_not=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71751</th>\n",
       "      <td>655</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>891585279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80493</th>\n",
       "      <td>748</td>\n",
       "      <td>208</td>\n",
       "      <td>4</td>\n",
       "      <td>879454522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>303</td>\n",
       "      <td>388</td>\n",
       "      <td>2</td>\n",
       "      <td>879544365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53233</th>\n",
       "      <td>463</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>877385414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91141</th>\n",
       "      <td>864</td>\n",
       "      <td>588</td>\n",
       "      <td>3</td>\n",
       "      <td>888887289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29759</th>\n",
       "      <td>274</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>878946612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88142</th>\n",
       "      <td>833</td>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>875122884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36759</th>\n",
       "      <td>338</td>\n",
       "      <td>168</td>\n",
       "      <td>3</td>\n",
       "      <td>879438225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90574</th>\n",
       "      <td>845</td>\n",
       "      <td>286</td>\n",
       "      <td>5</td>\n",
       "      <td>885409719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54452</th>\n",
       "      <td>486</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>879875441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id movie_id  rating  timestamp\n",
       "71751     655       52       3  891585279\n",
       "80493     748      208       4  879454522\n",
       "2655      303      388       2  879544365\n",
       "53233     463      111       2  877385414\n",
       "91141     864      588       3  888887289\n",
       "...       ...      ...     ...        ...\n",
       "29759     274       71       4  878946612\n",
       "88142     833      234       3  875122884\n",
       "36759     338      168       3  879438225\n",
       "90574     845      286       5  885409719\n",
       "54452     486      220       3  879875441\n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_test_user_item_interaction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 91/20000 [00:00<00:45, 437.18it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1581'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1581'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JIAN_A~1\\AppData\\Local\\Temp/ipykernel_10536/3269911532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrating_test_user_item_interaction_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Recommandation_System\\Matrix_Factorization.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, testdata)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtestdata\u001b[0m\u001b[0;31m：\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;31m，\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"yhat\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"MSE: {mean_squared_error(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\\nr2_score: {r2_score(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Recommandation_System\\Matrix_Factorization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(user, item)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtestdata\u001b[0m\u001b[0;31m：\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;31m，\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"yhat\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"MSE: {mean_squared_error(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\\nr2_score: {r2_score(y_true=testdata.iloc[:, 2], y_pred=testdata['yhat'])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Recommandation_System\\Matrix_Factorization.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, user_id, item_id)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0myhat_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_id_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0myhat_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[1;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    860\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[1;31m# This is an elided recursive call to iloc/loc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not applicable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3774\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3775\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3776\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3778\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1581'"
     ]
    }
   ],
   "source": [
    "model.evaluate(testdata=rating_test_user_item_interaction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM、FNN、IPNN、OPNN、CCPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:02<12:37,  2.53s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
<<<<<<< HEAD
    "num_features = 10\n",
=======
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
    "num_features_list = list(range(10, 101, 10))\n",
    "model = [\"FM\", \"FNN\", \"IPNN\", \"OPNN\", \"CCPM\", \"wide_and_deep\"]\n",
    "epochs = 100\n",
    "test_data_index_list = [0, 1, 2, 3, 4]\n",
    "test_data_index = 0\n",
    "learning_rate = 1e-2\n",
    "decay_ratio = 1e-3\n",
    "report = list()\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "for select_model, test_data_index, num_features in itertools.product(model, test_data_index_list, num_features_list):\n",
=======
    "for num_features, select_model, test_data_index in itertools.product(num_features_list, model, test_data_index_list):\n",
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
    "    train_each_iteration_loss = list()\n",
    "    train_loss_list = list()\n",
    "    test_total_result = list()\n",
    "    # 1. 將資料切割，並挑選出想要的組合\n",
    "    movielens_rating_train_user_id_onehotencoding, movielens_rating_test_user_id_onehotencoding,\\\n",
    "    movielens_rating_train_movie_id_onehotencoding, movielens_rating_test_movie_id_onehotencoding,\\\n",
    "    movielens_rating_train_user_age_onehotencoding, movielens_rating_test_user_age_onehotencoding,\\\n",
    "    movielens_rating_train_user_occupation_onehotencoding, movielens_rating_test_user_occupation_onehotencoding,\\\n",
    "    movielens_rating_train_movie_feature_data, movielens_rating_test_movie_feature_data,\\\n",
    "    movielens_rating_train_result_data, movielens_rating_test_result_data,\\\n",
    "    movielens_rating_train_user_item_interaction_data, movielens_rating_test_user_item_interaction_data,\\\n",
    "    movielens_rating_train_user_item_matrix_data = generate_input_data_movielens(movielens_rating_merge_data.drop(index=movielens_rating_index_group[test_data_index]), movielens_rating_merge_data.loc[movielens_rating_index_group[test_data_index], :], target=\"rating\")\n",
    "\n",
    "\n",
    "    # 2. 把所有東西包成dataloader\n",
    "    train_dataset = TensorDataset( torch.FloatTensor(movielens_rating_train_user_age_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_user_occupation_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_movie_feature_data.values),\n",
    "                                torch.FloatTensor(movielens_rating_train_result_data) )\n",
    "    test_dataset = TensorDataset(  torch.FloatTensor(movielens_rating_test_user_age_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_user_occupation_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_movie_feature_data.values),\n",
    "                                torch.FloatTensor(movielens_rating_test_result_data))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    # 3. 呼叫所需要的模型與設定Loss function\n",
    "    if select_model == \"FM\":\n",
    "        model = movielens_model.fm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")\n",
    "\n",
    "    elif select_model == \"FNN\":\n",
    "        model = movielens_model.fnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")\n",
    "    elif select_model == \"IPNN\":\n",
    "        model = movielens_model.ipnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")\n",
    "    elif select_model == \"OPNN\":\n",
    "        model = movielens_model.opnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")\n",
    "    elif select_model == \"CCPM\":\n",
    "        model = movielens_model.ccpm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")\n",
    "    elif select_model == \"wide_and_deep\":\n",
    "        model = movielens_model.wide_deep_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"regression\")                                     \n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=decay_ratio)\n",
    "\n",
    "    # 4. 模型訓練（要計算時間）\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "            yhat = model(user_age_feature=torch_user_age, \n",
    "                        user_occupation_feature=torch_user_occupation,\n",
    "                        movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "            loss = loss_func(yhat, torch_result)\n",
    "            train_loss += loss.item()\n",
    "            train_each_iteration_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss_list.append(train_loss)\n",
    "        # print(f\"===Epoch: {epoch}, Train loss: {train_loss}  ===\")\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # 5. 預測rating\n",
    "    start_predict_time = time.time()\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in test_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                        user_occupation_feature=torch_user_occupation,\n",
    "                        movie_genre_feature=torch_movie_genre)\n",
    "        test_total_result.extend(yhat.detach().numpy().flatten().tolist())\n",
    "    end_predict_time = time.time()\n",
    "\n",
    "    # 6. 計算模型評估\n",
    "    # y_true = movielens_rating_test_result_data；y_pred = test_total_result\n",
    "    # (1) 計算RMSE、(2) 計算R2 score、(3) 繪製散佈圖\n",
    "    rmse_result = mean_squared_error(y_true=movielens_rating_test_result_data, y_pred=test_total_result)\n",
    "    r2_score_result = r2_score(y_true=movielens_rating_test_result_data, y_pred=test_total_result)\n",
    "\n",
    "    # 7. 建立分析報表\n",
    "    # 欄位包含：模型、測試資料效能結果、訓練資料結果效能、特徵數量、測試次數、學習率、正規化比值、訓練圈數、訓練與預測時間\n",
    "    report.append([select_model, \"rating_prediction\", rmse_result, r2_score_result, [train_loss_list], num_features, test_data_index, learning_rate, epochs, decay_ratio, end_train_time-start_train_time, end_predict_time-start_train_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF in NeuCF、MLP in NeuCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
<<<<<<< HEAD
=======
    "num_features_list = list(range(10, 101, 10))\n",
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
    "model = [\"GMF_in_NeuCF\", \"MLP_in_NeuCF\"]\n",
    "select_model = \"GMF_in_NeuCF\"\n",
    "test_data_index_list = [0, 1, 2, 3, 4]\n",
    "test_data_index = 0\n",
    "num_features_list = list(range(10, 101, 1))\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "for select_model, test_data_index, num_features in itertools.product(model, test_data_index_list, num_features_list):\n",
=======
    "for num_features, select_model, test_data_index in itertools.product(num_features_list, model, test_data_index_list):\n",
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
    "    train_each_iteration_loss = list()\n",
    "    train_loss_list = list()\n",
    "    test_total_result = list()\n",
    "    # 1. 將資料切割，並挑選出想要的組合\n",
    "    movielens_rating_train_user_id_onehotencoding, movielens_rating_test_user_id_onehotencoding,\\\n",
    "    movielens_rating_train_movie_id_onehotencoding, movielens_rating_test_movie_id_onehotencoding,\\\n",
    "    movielens_rating_train_user_age_onehotencoding, movielens_rating_test_user_age_onehotencoding,\\\n",
    "    movielens_rating_train_user_occupation_onehotencoding, movielens_rating_test_user_occupation_onehotencoding,\\\n",
    "    movielens_rating_train_movie_feature_data, movielens_rating_test_movie_feature_data,\\\n",
    "    movielens_rating_train_result_data, movielens_rating_test_result_data,\\\n",
    "    movielens_rating_train_user_item_interaction_data, movielens_rating_test_user_item_interaction_data,\\\n",
    "    movielens_rating_train_user_item_matrix_data = generate_input_data_movielens(movielens_rating_merge_data.drop(index=movielens_rating_index_group[0]), movielens_rating_merge_data.loc[movielens_rating_index_group[0], :], target=\"rating\")\n",
    "\n",
    "    # 2. 把所有東西包成dataloader\n",
    "    train_dataset = TensorDataset( torch.FloatTensor(movielens_rating_train_user_id_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_movie_id_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_result_data) )\n",
    "    test_dataset = TensorDataset( torch.FloatTensor(movielens_rating_test_user_id_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_movie_id_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_result_data))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3. 呼叫所需要的模型與設定Loss function\n",
    "    if select_model == \"GMF_in_NeuCF\":\n",
    "        model = movielens_model.gmf_neucf_model( num_user=movielens_rating_train_user_id_onehotencoding.shape[1],\n",
    "                                num_item=movielens_rating_train_movie_id_onehotencoding.shape[1],\n",
    "                                num_features=num_features)\n",
    "    elif select_model == \"MLP_in_NeuCF\":\n",
    "        model = movielens_model.gmf_neucf_model( num_user=movielens_rating_train_user_id_onehotencoding.shape[1],\n",
    "                                num_item=movielens_rating_train_movie_id_onehotencoding.shape[1],\n",
    "                                num_features=num_features)\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "    # 4. 模型訓練（要計算時間）\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for torch_user, torch_movie, torch_result in train_dataloader:\n",
    "            yhat = model(user=torch_user,\n",
    "                        item=torch_movie)\n",
    "\n",
    "            loss = loss_func(yhat, torch_result)\n",
    "            train_loss += loss.item()\n",
    "            train_each_iteration_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss_list.append(train_loss)\n",
    "        print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # 5. 預測rating\n",
    "    start_train_time = time.time()\n",
    "    for torch_user, torch_movie, torch_result in test_dataloader:\n",
    "        yhat = model(user=torch_user,\n",
    "                    item=torch_movie)\n",
    "        test_total_result.extend(yhat.detach().numpy().flatten().tolist())\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # 6. 計算模型評估\n",
    "    # y_true = movielens_rating_test_result_data；y_pred = test_total_result\n",
    "    # (1) 計算RMSE、(2) 計算R2 score、(3) 繪製散佈圖\n",
    "    rmse_result = mean_squared_error(y_true=movielens_rating_test_result_data, y_pred=test_total_result)\n",
    "    r2_score_result = r2_score(y_true=movielens_rating_test_result_data, y_pred=test_total_result)\n",
    "\n",
    "    # 7. 建立分析報表\n",
    "    # 欄位包含：模型、測試資料效能結果、訓練資料結果效能、測試次數、學習率、正規化比值、訓練圈數、訓練與預測時間\n",
    "    report.append([select_model, \"rating_prediction\", rmse_result, r2_score_result, [train_loss_list], test_data_index, learning_rate, epochs, decay_ratio, end_train_time-start_train_time, end_predict_time-start_train_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"report//movielens_rating_each_model.xlsx\")\n",
    "pd.DataFrame(report, columns=[\"Model\", \"target\", \"RMSE\", \"R2_score\", \"train_loss_list\", \"test_group\", \"learning_rate\", \"epochs\", \"decay_ratio\", \"train_time\", \"predict_time\"]).to_excel(writer, index=None)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk_recall(testdata,\n",
    "                         one_user,\n",
    "                         user_column, \n",
    "                         item_column, \n",
    "                         true_result_column, \n",
    "                         topk=10):\n",
    "    \"\"\"\n",
    "    testdata: data.frame，同時有user_id, item_id, rating, yhat\n",
    "    \"\"\"\n",
    "    one_user_data = testdata[testdata[user_column] == one_user]\n",
    "\n",
    "    # 2. 針對某個User，挑出所有真實有評分的Item，以及挑出預測前十名的Item\n",
    "    true_item = list(one_user_data[one_user_data[true_result_column] == 1][item_column])\n",
    "    predict_item = list(one_user_data.sort_values(\"yhat\", ascending=True)[item_column][:10])\n",
    "\n",
    "    # 3. 計算所有有真實評分的Item的數量\n",
    "    length_true_item = len(true_item)\n",
    "\n",
    "    # 4. 計算預測與真實有相對應的資料\n",
    "    length_true_predict_match = sum([1 if i in predict_item else 0 for i in true_item])\n",
    "\n",
    "    # 5. 計算Recall\n",
    "    try:\n",
    "        topk_recall = length_true_predict_match / length_true_item\n",
    "    except:\n",
    "        topk_recall = 0\n",
    "    return topk_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(testdata,\n",
    "         one_user,\n",
    "         user_column,\n",
    "         item_column,\n",
    "         y_true,\n",
    "         y_pred):\n",
    "     # 1. 挑選出該user的資料\n",
    "     one_user_data = testdata[testdata[user_column] == one_user]\n",
    "\n",
    "     # 2. 以y_true排序→想法：使用者本身越有需求的，期望相關性越高\n",
    "     one_user_data = one_user_data.sort_values(by=y_true)\n",
    "\n",
    "     # 3. 把y_pred取出來\n",
    "     one_user_yhat = one_user_data[y_pred].values\n",
    "\n",
    "     # 4. 計算NDCG\n",
    "     dcg = one_user_yhat[0] + np.sum(one_user_yhat[1:] / np.log2(np.arange(2, one_user_yhat.size + 1)))\n",
    "     one_user_yhat_sort = sorted(one_user_yhat, reverse=True)\n",
    "     one_user_yhat_sort = np.array(one_user_yhat_sort)\n",
    "     dcg_max = one_user_yhat_sort[0] + np.sum(one_user_yhat_sort[1:] / np.log2(np.arange(2, one_user_yhat_sort.size + 1)))\n",
    "     return dcg / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM、FNN、IPNN、OPNN、CCPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===FM, 0===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.9 GiB for an array with shape (1586117, 1682) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JIAN_A~1\\AppData\\Local\\Temp/ipykernel_5460/898625368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmovielens_binary_train_result_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovielens_binary_test_result_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmovielens_binary_train_user_item_interaction_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovielens_binary_test_user_item_interaction_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mmovielens_binary_train_user_item_matrix_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_input_data_movielens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovielens_binary_merge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmovielens_binary_index_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovielens_binary_merge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmovielens_binary_index_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JIAN_A~1\\AppData\\Local\\Temp/ipykernel_5460/3115522912.py\u001b[0m in \u001b[0;36mgenerate_input_data_movielens\u001b[1;34m(rating_train_merge_data, rating_test_merge_data, target)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmovielens_rating_user_id_onehotencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrating_train_user_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_user_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mrating_train_movie_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_movie_id_onehotencoding\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmovielens_rating_movie_id_onehotencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrating_train_movie_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_movie_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrating_train_user_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_user_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JIAN_A~1\\AppData\\Local\\Temp/ipykernel_5460/3115522912.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmovielens_rating_user_id_onehotencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrating_train_user_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_user_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mrating_train_movie_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_movie_id_onehotencoding\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmovielens_rating_movie_id_onehotencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrating_train_movie_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_movie_id_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrating_train_user_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_test_user_id_onehotencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m         )\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python3.7.5\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.9 GiB for an array with shape (1586117, 1682) and data type float64"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_features_list = list(range(10, 101, 10))\n",
    "model = [\"FM\", \"FNN\", \"IPNN\", \"OPNN\", \"CCPM\", \"wide_and_deep\"]\n",
    "select_model = \"FNN\"\n",
    "epochs = 100\n",
    "test_data_index_list = [0, 1, 2, 3, 4]\n",
    "test_data_index = 0\n",
    "learning_rate = 1e-2\n",
    "decay_ratio = 1e-3\n",
    "report = list()\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "for select_model, test_data_index, num_features in itertools.product(model, test_data_index_list, num_features_list):\n",
    "    train_each_iteration_loss = list()\n",
    "    train_loss_list = list()\n",
    "    test_total_result = list()\n",
    "    print(f\"==={select_model}, {test_data_index}===\")\n",
    "    # 1. 將資料切割，並挑選出想要的組合\n",
    "    movielens_binary_train_user_id_onehotencoding, movielens_binary_test_user_id_onehotencoding,\\\n",
    "    movielens_binary_train_movie_id_onehotencoding, movielens_binary_test_movie_id_onehotencoding,\\\n",
    "    movielens_binary_train_user_age_onehotencoding, movielens_binary_test_user_age_onehotencoding,\\\n",
    "    movielens_binary_train_user_occupation_onehotencoding, movielens_binary_test_user_occupation_onehotencoding,\\\n",
    "    movielens_binary_train_movie_feature_data, movielens_binary_test_movie_feature_data,\\\n",
    "    movielens_binary_train_result_data, movielens_binary_test_result_data,\\\n",
    "    movielens_binary_train_user_item_interaction_data, movielens_binary_test_user_item_interaction_data,\\\n",
    "    movielens_binary_train_user_item_matrix_data = generate_input_data_movielens(movielens_binary_merge_data.drop(index=movielens_binary_index_group[test_data_index]), movielens_binary_merge_data.loc[movielens_binary_index_group[test_data_index], :], target=\"value\")\n",
    "\n",
    "\n",
    "    # 2. 把所有東西包成dataloader\n",
    "    train_dataset = TensorDataset( torch.FloatTensor(movielens_rating_train_user_age_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_user_occupation_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_train_movie_feature_data.astype(\"int\").values),\n",
    "                                torch.FloatTensor(movielens_rating_train_result_data) )\n",
    "    test_dataset = TensorDataset(  torch.FloatTensor(movielens_rating_test_user_age_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_user_occupation_onehotencoding),\n",
    "                                torch.FloatTensor(movielens_rating_test_movie_feature_data.astype(\"int\").values),\n",
    "                                torch.FloatTensor(movielens_rating_test_result_data))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    # 3. 呼叫所需要的模型與設定Loss function\n",
    "    if select_model == \"FM\":\n",
    "        model = movielens_model.fm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")\n",
    "\n",
    "    elif select_model == \"FNN\":\n",
    "        model = movielens_model.fnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")\n",
    "    elif select_model == \"IPNN\":\n",
    "        model = movielens_model.ipnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")\n",
    "    elif select_model == \"OPNN\":\n",
    "        model = movielens_model.opnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")\n",
    "    elif select_model == \"CCPM\":\n",
    "        model = movielens_model.ccpm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")\n",
    "    elif select_model == \"wide_and_deep\":\n",
    "        model = movielens_model.wide_deep_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                        num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                        num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                        num_features=num_features,\n",
    "                                        methods = \"classification\")                                     \n",
    "\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGC(model.parameters(), lr=learning_rate, weight_decay=decay_ratio)\n",
    "\n",
    "    # 4. 模型訓練（要計算時間）\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "            yhat = model(user_age_feature=torch_user_age, \n",
    "                        user_occupation_feature=torch_user_occupation,\n",
    "                        movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "            loss = loss_func(yhat, torch_result)\n",
    "            train_loss += loss.item()\n",
    "            train_each_iteration_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss_list.append(train_loss)\n",
    "        # print(f\"===Epoch: {epoch}, Train loss: {train_loss}  ===\")\n",
    "    end_train_time = time.time()\n",
    "\n",
    "    # 5. 預測rating\n",
    "    start_predict_time = time.time()\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in test_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                        user_occupation_feature=torch_user_occupation,\n",
    "                        movie_genre_feature=torch_movie_genre)\n",
    "        test_total_result.extend(yhat[0, :].detach().numpy().flatten().tolist())\n",
    "    end_predict_time = time.time()\n",
    "\n",
    "    # 6. 計算模型評估\n",
    "    # y_true = movielens_binary_test_result_data；y_pred = test_total_result\n",
    "    # (1) 計算Recall@10、(2) 計算Accuracy、(3) 計算Precision、(4) 計算Recall、(5) 繪製ROC Curve、(6) 繪製PRC Curve\n",
    "\n",
    "    # 把預測結果鑲嵌至testdata\n",
    "    movielens_binary_test_user_item_interaction_data[\"yhat\"] = np.array(test_total_result)\n",
    "    ### Recall@10 #######\n",
    "    # 1. 建立User清單\n",
    "    binary_test_user_id = list(movielens_binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
    "\n",
    "    one_user = [0]\n",
    "    all_recall_list = list(map(lambda x: evaluate_topk_recall(testdata=movielens_binary_test_user_item_interaction_data, \n",
    "                                                            one_user=x,\n",
    "                                                            user_column=\"user_id\",\n",
    "                                                            item_column=\"movie_id\",\n",
    "                                                            true_result_column=\"value\"), binary_test_user_id))\n",
    "    # 6. 將所有Recall平均\n",
    "    final_recall = np.array(all_recall_list).mean()\n",
    "    ############\n",
    "\n",
    "    ### NDCG@10 ###\n",
    "    # 1. 建立User清單\n",
    "    binary_test_user_id = list(movielens_binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
    "\n",
    "    one_user = [0]\n",
    "    all_ndcg_list = list(map(lambda x: NDCG(testdata=movielens_binary_test_user_item_interaction_data, \n",
    "                                                            one_user=x,\n",
    "                                                            user_column=\"user_id\",\n",
    "                                                            item_column=\"movie_id\",\n",
    "                                                            y_true=\"value\",\n",
    "                                                            y_pred=\"yhat\"), binary_test_user_id))\n",
    "    # 6. 將所有Recall平均\n",
    "    final_ndcg = np.array(all_ndcg_list).mean()\n",
    "    ###############\n",
    "    \n",
    "    # 7. 建立分析報表\n",
    "    # 欄位包含：模型、測試資料效能結果、訓練資料結果效能、特徵數量、測試次數、學習率、正規化比值、訓練圈數、訓練與預測時間\n",
    "    report.append([select_model, final_recall, final_ndcg, [train_loss_list], num_features, test_data_index, learning_rate, epochs, decay_ratio, end_train_time-start_train_time, end_predict_time-start_train_time])"
=======
    "for num_features in num_features_list:\n",
    "    for select_model, test_data_index in itertools.product(model, test_data_index_list):\n",
    "        train_each_iteration_loss = list()\n",
    "        train_loss_list = list()\n",
    "        test_total_result = list()\n",
    "        print(f\"==={select_model}, {test_data_index}===\")\n",
    "        # 1. 將資料切割，並挑選出想要的組合\n",
    "        movielens_binary_train_user_id_onehotencoding, movielens_binary_test_user_id_onehotencoding,\\\n",
    "        movielens_binary_train_movie_id_onehotencoding, movielens_binary_test_movie_id_onehotencoding,\\\n",
    "        movielens_binary_train_user_age_onehotencoding, movielens_binary_test_user_age_onehotencoding,\\\n",
    "        movielens_binary_train_user_occupation_onehotencoding, movielens_binary_test_user_occupation_onehotencoding,\\\n",
    "        movielens_binary_train_movie_feature_data, movielens_binary_test_movie_feature_data,\\\n",
    "        movielens_binary_train_result_data, movielens_binary_test_result_data,\\\n",
    "        movielens_binary_train_user_item_interaction_data, movielens_binary_test_user_item_interaction_data,\\\n",
    "        movielens_binary_train_user_item_matrix_data = generate_input_data_movielens(movielens_binary_merge_data.drop(index=movielens_binary_index_group[test_data_index]), movielens_binary_merge_data.loc[movielens_binary_index_group[test_data_index], :], target=\"value\")\n",
    "\n",
    "\n",
    "        # 2. 把所有東西包成dataloader\n",
    "        train_dataset = TensorDataset( torch.FloatTensor(movielens_rating_train_user_age_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_rating_train_user_occupation_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_rating_train_movie_feature_data.values),\n",
    "                                    torch.FloatTensor(movielens_rating_train_result_data) )\n",
    "        test_dataset = TensorDataset(  torch.FloatTensor(movielens_rating_test_user_age_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_rating_test_user_occupation_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_rating_test_movie_feature_data.values),\n",
    "                                    torch.FloatTensor(movielens_rating_test_result_data))\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "        # 3. 呼叫所需要的模型與設定Loss function\n",
    "        if select_model == \"FM\":\n",
    "            model = movielens_model.fm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")\n",
    "\n",
    "        elif select_model == \"FNN\":\n",
    "            model = movielens_model.fnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")\n",
    "        elif select_model == \"IPNN\":\n",
    "            model = movielens_model.ipnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")\n",
    "        elif select_model == \"OPNN\":\n",
    "            model = movielens_model.opnn_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")\n",
    "        elif select_model == \"CCPM\":\n",
    "            model = movielens_model.ccpm_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")\n",
    "        elif select_model == \"wide_and_deep\":\n",
    "            model = movielens_model.wide_deep_model(num_user_age=movielens_rating_train_user_age_onehotencoding.shape[1], \n",
    "                                            num_user_occupation=movielens_rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                                            num_movie_genre=movielens_rating_train_movie_feature_data.values.shape[1],\n",
    "                                            num_features=num_features,\n",
    "                                            methods = \"classification\")                                     \n",
    "\n",
    "        loss_func = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGC(model.parameters(), lr=learning_rate, weight_decay=decay_ratio)\n",
    "\n",
    "        # 4. 模型訓練（要計算時間）\n",
    "        start_train_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "                yhat = model(user_age_feature=torch_user_age, \n",
    "                            user_occupation_feature=torch_user_occupation,\n",
    "                            movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "                loss = loss_func(yhat, torch_result)\n",
    "                train_loss += loss.item()\n",
    "                train_each_iteration_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_loss_list.append(train_loss)\n",
    "            # print(f\"===Epoch: {epoch}, Train loss: {train_loss}  ===\")\n",
    "        end_train_time = time.time()\n",
    "\n",
    "        # 5. 預測rating\n",
    "        start_predict_time = time.time()\n",
    "        for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in test_dataloader:\n",
    "            yhat = model(user_age_feature=torch_user_age, \n",
    "                            user_occupation_feature=torch_user_occupation,\n",
    "                            movie_genre_feature=torch_movie_genre)\n",
    "            test_total_result.extend(yhat.detach().numpy().flatten().tolist())\n",
    "        end_predict_time = time.time()\n",
    "\n",
    "        # 6. 計算模型評估\n",
    "        # y_true = movielens_binary_test_result_data；y_pred = test_total_result\n",
    "        # (1) 計算Recall@10、(2) 計算Accuracy、(3) 計算Precision、(4) 計算Recall、(5) 繪製ROC Curve、(6) 繪製PRC Curve\n",
    "\n",
    "        # 把預測結果鑲嵌至testdata\n",
    "        movielens_binary_test_user_item_interaction_data[\"yhat\"] = np.array(test_total_result)\n",
    "        ### Recall@10 #######\n",
    "        # 1. 建立User清單\n",
    "        binary_test_user_id = list(movielens_binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
    "\n",
    "        one_user = [0]\n",
    "        all_recall_list = list(map(lambda x: evaluate_topk_recall(testdata=movielens_binary_test_user_item_interaction_data, \n",
    "                                                                one_user=x,\n",
    "                                                                user_column=\"user_id\",\n",
    "                                                                item_column=\"movie_id\",\n",
    "                                                                true_result_column=\"value\"), binary_test_user_id))\n",
    "        # 6. 將所有Recall平均\n",
    "        final_recall = np.array(all_recall_list).mean()\n",
    "        ############\n",
    "\n",
    "        ### NDCG@10 ###\n",
    "        # 1. 建立User清單\n",
    "        binary_test_user_id = list(movielens_binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
    "\n",
    "        one_user = [0]\n",
    "        all_ndcg_list = list(map(lambda x: NDCG(testdata=movielens_binary_test_user_item_interaction_data, \n",
    "                                                                one_user=x,\n",
    "                                                                user_column=\"user_id\",\n",
    "                                                                item_column=\"movie_id\",\n",
    "                                                                y_true=\"value\",\n",
    "                                                                y_pred=\"yhat\"), binary_test_user_id))\n",
    "        # 6. 將所有Recall平均\n",
    "        final_ndcg = np.array(all_ndcg_list).mean()\n",
    "        ###############\n",
    "        \n",
    "        # 7. 建立分析報表\n",
    "        # 欄位包含：模型、測試資料效能結果、訓練資料結果效能、特徵數量、測試次數、學習率、正規化比值、訓練圈數、訓練與預測時間\n",
    "        report.append([select_model, \"binary_prediction\", final_recall, final_ndcg, [train_loss_list], num_features, test_data_index, learning_rate, epochs, decay_ratio, end_train_time-start_train_time, end_predict_time-start_train_time])"
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF in NeuCF、MLP in NeuCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "num_features_list = list(range(10, 101, 10))\n",
    "model = [\"GMF_in_NeuCF\", \"MLP_in_NeuCF\"]\n",
    "select_model = \"GMF_in_NeuCF\"\n",
    "test_data_index_list = [0, 1, 2, 3, 4]\n",
    "test_data_index = 0\n",
    "\n",
    "for num_features in num_features_list:\n",
    "    for select_model, test_data_index in itertools.product(model, test_data_index_list):\n",
    "        train_each_iteration_loss = list()\n",
    "        train_loss_list = list()\n",
    "        test_total_result = list()\n",
    "        # 1. 將資料切割，並挑選出想要的組合\n",
    "        movielens_binary_train_user_id_onehotencoding, movielens_binary_test_user_id_onehotencoding,\\\n",
    "        movielens_binary_train_movie_id_onehotencoding, movielens_binary_test_movie_id_onehotencoding,\\\n",
    "        movielens_binary_train_user_age_onehotencoding, movielens_binary_test_user_age_onehotencoding,\\\n",
    "        movielens_binary_train_user_occupation_onehotencoding, movielens_binary_test_user_occupation_onehotencoding,\\\n",
    "        movielens_binary_train_movie_feature_data, movielens_binary_test_movie_feature_data,\\\n",
    "        movielens_binary_train_result_data, movielens_binary_test_result_data,\\\n",
    "        movielens_binary_train_user_item_interaction_data, movielens_binary_test_user_item_interaction_data,\\\n",
    "        movielens_binary_train_user_item_matrix_data = generate_input_data_movielens(movielens_binary_merge_data.drop(index=movielens_binary_index_group[test_data_index]), movielens_binary_merge_data.loc[movielens_binary_index_group[test_data_index], :], target=\"value\")\n",
    "\n",
    "        # 2. 把所有東西包成dataloader\n",
    "        train_dataset = TensorDataset( torch.FloatTensor(movielens_binary_train_user_id_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_binary_train_movie_id_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_binary_train_result_data) )\n",
    "        test_dataset = TensorDataset( torch.FloatTensor(movielens_binary_train_user_id_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_binary_train_movie_id_onehotencoding),\n",
    "                                    torch.FloatTensor(movielens_binary_train_result_data))\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 3. 呼叫所需要的模型與設定Loss function\n",
    "        if select_model == \"GMF_in_NeuCF\":\n",
    "            model = movielens_model.gmf_neucf_model( num_user=movielens_binary_train_user_id_onehotencoding.shape[1],\n",
    "                                    num_item=movielens_binary_train_movie_id_onehotencoding.shape[1],\n",
    "                                    num_features=num_features,\n",
    "                                    methods=\"classification\")\n",
    "        elif select_model == \"MLP_in_NeuCF\":\n",
    "            model = movielens_model.gmf_neucf_model( num_user=movielens_binary_train_user_id_onehotencoding.shape[1],\n",
    "                                    num_item=movielens_binary_train_movie_id_onehotencoding.shape[1],\n",
    "                                    num_features=num_features,\n",
    "                                    methods=\"classification\")\n",
    "\n",
    "        loss_func = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "        # 4. 模型訓練（要計算時間）\n",
    "        start_train_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            for torch_user, torch_movie, torch_result in train_dataloader:\n",
    "                yhat = model(user=torch_user,\n",
    "                            item=torch_movie)\n",
    "\n",
    "                loss = loss_func(yhat, torch_result)\n",
    "                train_loss += loss.item()\n",
    "                train_each_iteration_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_loss_list.append(train_loss)\n",
    "            print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")\n",
    "        end_train_time = time.time()\n",
    "\n",
    "        # 5. 預測rating\n",
    "        start_train_time = time.time()\n",
    "        for torch_user, torch_movie, torch_result in test_dataloader:\n",
    "            yhat = model(user=torch_user,\n",
    "                        item=torch_movie)\n",
    "            test_total_result.extend(yhat.detach().numpy().flatten().tolist())\n",
    "        end_train_time = time.time()\n",
    "\n",
    "        # 6. 將所有Recall平均\n",
    "        final_recall = np.array(all_recall_list).mean()\n",
    "        ############\n",
    "\n",
<<<<<<< HEAD
    "    # 5. 預測rating\n",
    "    start_train_time = time.time()\n",
    "    for torch_user, torch_movie, torch_result in test_dataloader:\n",
    "        yhat = model(user=torch_user,\n",
    "                    item=torch_movie)\n",
    "        test_total_result.extend(yhat[:, 0].detach().numpy().flatten().tolist())\n",
    "    end_train_time = time.time()\n",
=======
    "        ### NDCG@10 ###\n",
    "        # 1. 建立User清單\n",
    "        binary_test_user_id = list(movielens_binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
>>>>>>> 6cd43d23a2829bb9be31815d396875c65022cfc1
    "\n",
    "        one_user = [0]\n",
    "        all_ndcg_list = list(map(lambda x: NDCG(testdata=movielens_binary_test_user_item_interaction_data, \n",
    "                                                                one_user=x,\n",
    "                                                                user_column=\"user_id\",\n",
    "                                                                item_column=\"movie_id\",\n",
    "                                                                y_true=\"value\",\n",
    "                                                                y_pred=\"yhat\"), binary_test_user_id))\n",
    "        # 6. 將所有Recall平均\n",
    "        final_ndcg = np.array(all_ndcg_list).mean()\n",
    "        ###############\n",
    "        \n",
    "        # 7. 建立分析報表\n",
    "        # 欄位包含：模型、測試資料效能結果、訓練資料結果效能、特徵數量、測試次數、學習率、正規化比值、訓練圈數、訓練與預測時間\n",
    "        report.append([select_model, \"binary_prediction\", final_recall, final_ndcg, [train_loss_list], num_features, test_data_index, learning_rate, epochs, decay_ratio, end_train_time-start_train_time, end_predict_time-start_train_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"report//movielens_binary_each_model.xlsx\")\n",
    "pd.DataFrame(report, columns=[\"Model\", \"target\", \"TopK_recall\", \"TopK_NDCG\", \"train_loss_list\", \"test_group\", \"learning_rate\", \"epochs\", \"decay_ratio\", \"train_time\", \"predict_time\"]).to_excel(writer, index=None)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Douban book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM、FNN、IPNN、OPNN、CCPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 10\n",
    "model = [\"FM\", \"FNN\", \"IPNN\", \"OPNN\", \"CCPM\", \"wide_and_deep\"]\n",
    "epochs = 100\n",
    "test_data_index_list = [0, 1, 2, 3, 4]\n",
    "test_data_index = 0\n",
    "learning_rate = 1e-2\n",
    "decay_ratio = 1e-3\n",
    "report = list()\n",
    "\n",
    "\n",
    "# 1. 生成前處理後的輸入資料\n",
    "book_rating_train_user_id_onehotencoding, book_rating_test_user_id_onehotencoding,\\\n",
    "book_rating_train_book_id_onehotencoding, book_rating_test_book_id_onehotencoding,\\\n",
    "book_rating_train_book_author_onehotencoding, book_rating_test_book_author_onehotencoding,\\\n",
    "book_rating_train_book_publisher_onehotencoding, book_rating_test_book_publisher_onehotencoding,\\\n",
    "book_rating_train_book_year_onehotencoding, book_rating_test_book_year_onehotencoding,\\\n",
    "book_rating_train_user_feature_data, book_rating_test_user_feature_data,\\\n",
    "book_rating_train_result_data, book_rating_test_result_data,\\\n",
    "book_rating_train_user_item_interaction_data, book_rating_train_user_item_interaction_data,\\\n",
    "book_train_user_item_matrix_data = generate_input_data_douban_book(book_rating_merge_data.drop(index=book_rating_index_group[one_group]), book_rating_merge_data.loc[book_rating_index_group[one_group], :], target=\"rating\")\n",
    "\n",
    "# 2. 把所有東西包成dataloader\n",
    "train_dataset = TensorDataset( torch.FloatTensor(book_rating_train_user_feature_data.values),\n",
    "                               torch.FloatTensor(book_rating_train_book_author_onehotencoding),\n",
    "                               torch.FloatTensor(book_rating_train_book_publisher_onehotencoding),\n",
    "                               torch.FloatTensor(book_rating_train_book_year_onehotencoding),\n",
    "                               torch.FloatTensor(book_rating_train_result_data) )\n",
    "test_dataset = TensorDataset(  torch.FloatTensor(book_rating_test_user_feature_data.values),\n",
    "                               torch.FloatTensor(book_rating_test_book_author_onehotencoding),\n",
    "                               torch.FloatTensor(book_rating_test_book_publisher_onehotencoding),\n",
    "                               torch.FloatTensor(book_rating_test_book_year_onehotencoding),\n",
    "                               torch.FloatTensor(rating_test_result_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# 3. 呼叫所需要的模型與設定Loss function\n",
    "if select_model == \"FM\":\n",
    "    model = book_model.fm_model(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "elif select_model == \"FNN\":\n",
    "    model = book_model.fnn_model(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "elif select_model == \"IPNN\":\n",
    "    model = book_model.ipnn_model(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "elif select_model == \"OPNN\":\n",
    "    model = book_model.opnn_model(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "elif select_model == \"CCPM\":\n",
    "    model = book_model.ccpm_model(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "elif select_model == \"wide_and_deep\":\n",
    "    model = book_model.wide_and_deep(num_user_group=book_rating_train_user_feature_data.shape[1],\n",
    "                                num_book_author=book_rating_train_book_author_onehotencoding.shape[1],\n",
    "                                num_book_publisher=book_rating_train_book_publisher_onehotencoding.shape[1],\n",
    "                                num_book_year=book_rating_train_book_year_onehotencoding.shape[1],\n",
    "                                num_features=num_features,\n",
    "                                methods = \"regression\")\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "# 4. 模型訓練（要計算時間）\n",
    "start_train_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_group, torch_book_author, torch_book_publisher, torch_book_year, torch_result in train_dataloader:\n",
    "        yhat = model(user_group_feature=torch_user_group,\n",
    "                     book_author_feature = torch_book_author,\n",
    "                     book_publisher_feature = torch_book_publisher,\n",
    "                     book_year_feature = torch_book_year)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result)\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")\n",
    "end_train_time = time.time()\n",
    "\n",
    "# 5. 預測rating\n",
    "start_predict_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_group, torch_book_author, torch_book_publisher, torch_book_year, torch_result in train_dataloader:\n",
    "        yhat = model(user_group_feature=torch_user_group,\n",
    "                     book_author_feature = torch_book_author,\n",
    "                     book_publisher_feature = torch_book_publisher,\n",
    "                     book_year_feature = torch_book_year)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result)\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")\n",
    "end_predict_time = time.time()\n",
    "\n",
    "# 6. 模型評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMF in NeuCF、MLP in NeuCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 10\n",
    "\n",
    "# 1. 把所有東西包成dataloader\n",
    "train_dataset = TensorDataset( torch.FloatTensor(binary_train_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(binary_train_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(binary_train_movie_feature_data.values),\n",
    "                               torch.FloatTensor(binary_train_result_data) )\n",
    "test_dataset = TensorDataset(  torch.FloatTensor(binary_test_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(binary_test_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(binary_test_movie_feature_data.values),\n",
    "                               torch.FloatTensor(binary_test_result_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "# 2. 呼叫模型與設定Loss function\n",
    "model = fm_model(num_user_age=binary_train_user_age_onehotencoding.shape[1], \n",
    "                 num_user_occupation=binary_train_user_occupation_onehotencoding.shape[1], \n",
    "                 num_movie_genre=binary_train_movie_feature_data.values.shape[1],\n",
    "                 num_features=num_features,\n",
    "                 methods = \"classification\")\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "epochs = 30\n",
    "\n",
    "train_each_iteration_loss = list()\n",
    "train_loss_list = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                     user_occupation_feature=torch_user_occupation,\n",
    "                     movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result.reshape((-1, 1)))\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "all_yhat = list()\n",
    "test_loss = 0.0\n",
    "\n",
    "for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in test_dataloader:\n",
    "    yhat = model(user_age_feature=torch_user_age, \n",
    "                    user_occupation_feature=torch_user_occupation,\n",
    "                    movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "    loss = loss_func(yhat, torch_result.reshape((-1, 1)))\n",
    "    test_loss += loss.item()\n",
    "    all_yhat.extend(yhat[:, 0].detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk_recall(testdata,\n",
    "                         one_user,\n",
    "                         user_column, \n",
    "                         item_column, \n",
    "                         true_result_column, \n",
    "                         topk=10):\n",
    "    \"\"\"\n",
    "    testdata: data.frame，同時有user_id, item_id, rating, yhat\n",
    "    \"\"\"\n",
    "    one_user_data = binary_test_user_item_interaction_data[binary_test_user_item_interaction_data[user_column] == one_user]\n",
    "\n",
    "    # 2. 針對某個User，挑出所有真實有評分的Item，以及挑出預測前十名的Item\n",
    "    true_item = list(one_user_data[one_user_data[true_result_column] == 1][item_column])\n",
    "    predict_item = list(one_user_data.sort_values(\"yhat\", ascending=True)[item_column][:10])\n",
    "\n",
    "    # 3. 計算所有有真實評分的Item的數量\n",
    "    length_true_item = len(true_item)\n",
    "\n",
    "    # 4. 計算預測與真實有相對應的資料\n",
    "    length_true_predict_match = sum([1 if i in predict_item else 0 for i in true_item])\n",
    "\n",
    "    # 5. 計算Recall\n",
    "    try:\n",
    "        topk_recall = length_true_predict_match / length_true_item\n",
    "    except:\n",
    "        topk_recall = 0\n",
    "    return topk_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# Recall@10\n",
    "\n",
    "# 0. 把預測結果鑲嵌至testdata\n",
    "binary_test_user_item_interaction_data[\"yhat\"] = np.array(all_yhat)\n",
    "\n",
    "# 1. 建立User清單\n",
    "binary_test_user_id = list(binary_test_user_item_interaction_data[\"user_id\"].unique())\n",
    "\n",
    "one_user = [0]\n",
    "all_recall_list = list(map(lambda x: evaluate_topk_recall(testdata=binary_test_user_item_interaction_data, \n",
    "                                                          one_user=x,\n",
    "                                                          user_column=\"user_id\",\n",
    "                                                          item_column=\"movie_id\",\n",
    "                                                          true_result_column=\"value\"), binary_test_user_id))\n",
    "\n",
    "# 6. 將所有Recall平均\n",
    "final_recall = np.array(all_recall_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 10\n",
    "\n",
    "# 1. 把所有東西包成dataloader\n",
    "train_dataset = TensorDataset( torch.FloatTensor(rating_train_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(rating_train_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(rating_train_movie_feature_data.values),\n",
    "                               torch.FloatTensor(rating_train_result_data) )\n",
    "test_dataset = TensorDataset(  torch.FloatTensor(rating_test_user_age_onehotencoding),\n",
    "                               torch.FloatTensor(rating_test_user_occupation_onehotencoding),\n",
    "                               torch.FloatTensor(rating_test_movie_feature_data.values),\n",
    "                               torch.FloatTensor(rating_test_result_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# 2. 呼叫模型與設定Loss function\n",
    "model = wide_deep_model(num_user_age=rating_train_user_age_onehotencoding.shape[1], \n",
    "                 num_user_occupation=rating_train_user_occupation_onehotencoding.shape[1], \n",
    "                 num_movie_genre=rating_train_movie_feature_data.values.shape[1],\n",
    "                 num_features=num_features,\n",
    "                 methods = \"regression\")\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0, Train Loss: 2054.7890256643295\n",
      "=== Epoch: 1, Train Loss: 845.0409094691277\n",
      "=== Epoch: 2, Train Loss: 829.567675113678\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "train_each_iteration_loss = list()\n",
    "train_loss_list = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for torch_user_age, torch_user_occupation, torch_movie_genre, torch_result in train_dataloader:\n",
    "        yhat = model(user_age_feature=torch_user_age, \n",
    "                     user_occupation_feature=torch_user_occupation,\n",
    "                     movie_genre_feature=torch_movie_genre)\n",
    "\n",
    "        loss = loss_func(yhat, torch_result)\n",
    "        train_loss += loss.item()\n",
    "        train_each_iteration_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss_list.append(train_loss)\n",
    "    print(f\"=== Epoch: {epoch}, Train Loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5cf5fa73e76fbdd1036fecfade4ba56213682791ca6f80f15a6630a2bc06098"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
